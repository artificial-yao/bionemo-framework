{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train MolMIM from scratch on your own data using the BioNeMo Framework\n",
    "\n",
    "The purpose of this tutorial is to provide an example use case of training MolMIM model using the BioNeMo framework. \n",
    "\n",
    "#### Demo objectives:\n",
    "- Learn how to prepare your own train, validation and test datasets for MolMIM training -> data processing steps including input validation, deduplication, filtering based on tokenizer vocabulary, dataset splitting\n",
    "- Train a MolMIM model from scratch (highlighting config options for customisable training)\n",
    "- Continue training an existing MolMIM model checkpoint\n",
    "\n",
    "**Note:** this notebook was developed and tested for BioNeMo framework container 1.7    \n",
    "Tested GPUs: A1000, A6000 (total notebook runtime using single GPU ~2 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview - MolMIM model\n",
    "\n",
    "MolMIM is a probabilistic auto-encoder for small molecule drug discovery, trained with Mutual Information Machine (MIM) learning. It provides a fixed size embedding for each molecule and produces a latent space from which samples can be drawn and novel SMILES strings generated from a specific starting molecule. By using optimizers in tandem with MolMIM, specific properties of the molecules can be optimized and novel derivatives generated. For more information, we direct the reader to [Reidenbach, et al. (2023)](https://arxiv.org/pdf/2208.09016).\n",
    "\n",
    "\n",
    "Architecture (see schematic below):\n",
    "- Perceiver Encoder \n",
    "- Transformer Decoder\n",
    "- Latent MLP heads for latent variable characterization\n",
    "\n",
    "\n",
    "<img src=\"../images/MolMIM_model.png\" width=1000 alt=\"Schematic of MolMIM model\" />\n",
    "\n",
    "\n",
    "Current MolMIM models were pretrained using only molecules that conform to Lipinski's rule of 5, here we will give an example of how you could train a custom model on molecules of your choice, without filtering using the Rule of 5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Assumptions\n",
    "\n",
    "This tutorial assumes that the user has access to BioNeMo framework container and GPU compute, please check the [Getting Started](../index.md) section for more information.  \n",
    "\n",
    "All model training related commands should be executed inside the BioNeMo docker container.\n",
    "\n",
    "**Note:** The interactive job launch example shown here using the Jupyter Lab interface is intended for initial user experience/trial runs. It is **strongly** advised to launch the model training jobs using the launch script as a part of the ``ngc batch run`` command, as mentioned in [Access and Startup](../access-startup.md).    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we install and import all our required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install PyTDC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries \n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from typing import Literal \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Importing libraries to download and split datasets from the Therapeutic Data Commons https://tdcommons.ai/\n",
    "from tdc.single_pred import Tox\n",
    "from tdc.base_dataset import DataLoader\n",
    "\n",
    "from nemo.collections.common.tokenizers.regex_tokenizer import RegExTokenizer\n",
    "\n",
    "bionemo_home = os.environ['BIONEMO_HOME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset preparation\n",
    "\n",
    "Here we go through data preparation steps for an example dataset from the [Therapeutic Data Commons](https://tdcommons.ai/): a regression dataset - the percent inhibition of the human ether-à-go-go related gene (hERG) channel by a given compound at a 10 µM concentration. This dataset contains a total of 306892 molecules.\n",
    "\n",
    "#### Download the dataset\n",
    "First we download the dataset, and perform some basic data processing, for example canonicalising the SMILES strings and dropping duplicate entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalise_smiles(smiles: str) -> str:\n",
    "    \"\"\"Returns the canonical SMILES string for the input SMILES string \n",
    "    or np.nan if it was not possible to generate a valid mol object from the input string\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    return np.nan if mol is None else Chem.MolToSmiles(mol)\n",
    "\n",
    "# Specify dataset for download\n",
    "dataset_name = 'herg_central'\n",
    "task = 'hERG_at_10uM'\n",
    "print(f\"Preparing raw {dataset_name} dataset from TD Commons, filtered for task: {task}.\")\n",
    "data = Tox(name = dataset_name, label_name = task)\n",
    "data_df = data.get_data()\n",
    "\n",
    "# Take only the first 10,000 molecules in order to reduce tutorial run time\n",
    "data_df = data_df[:10000]\n",
    "\n",
    "print(\"Generating canonical SMILES strings from the provided SMILES...\")\n",
    "data_df[\"canonical_smiles\"] = data_df[\"Drug\"].map(canonicalise_smiles)\n",
    "print(\"Dropping duplicate molecules (first instance kept)...\")\n",
    "unique_df = data_df.drop_duplicates(subset=[\"canonical_smiles\"])\n",
    "print(f\"{len(data_df) - len(unique_df)} duplicates removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter for vocabulary compliance\n",
    "Next, we exclude SMILES strings exceeding the model's maximum token limit and molecules with tokens absent from the model's vocabulary. To accomplish this, we import the model's existing vocabulary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token_length = 126\n",
    "# Note: the maximum token length generated from the smiles string should be 2 less than the max_seq_length specified in the model config. \n",
    "# This is to account for the extra tokens <BOS> and <EOS>\n",
    "\n",
    "def vocab_compliance_check(smiles: str, tokenizer: RegExTokenizer, max_token_length: int) -> bool:\n",
    "    \"\"\"Checks if the SMILES string only contains vocabulary in the tokenizer's vocabulary\n",
    "    and if the token length is less than or equal to `max_token_length\"\"\"\n",
    "    tokens = tokenizer.text_to_tokens(smiles)\n",
    "    vocab_allowed = tokenizer.vocab.keys()\n",
    "    return set(tokens).issubset(set(vocab_allowed)) and len(tokens) <= max_token_length\n",
    "\n",
    "model_name = \"molmim\"\n",
    "print(f\"Filtering out molecules which are not present in the {model_name} tokenizer vocabulary or with max token length greater than {max_token_length}...\")\n",
    "tokenizer_path = bionemo_home + \"/tokenizers/molecule/{model_name}/vocab/{model_name}.{extension}\"\n",
    "tokenizer = RegExTokenizer().load_tokenizer(regex_file=tokenizer_path.format(model_name=model_name, extension=\"model\"), vocab_file=tokenizer_path.format(model_name=model_name, extension=\"vocab\"))\n",
    "unique_df[\"vocab_compliant\"] = unique_df[\"canonical_smiles\"].apply(lambda smi: vocab_compliance_check(smi, tokenizer, max_token_length))\n",
    "# Select only molecules which are vocab compliant\n",
    "filtered_df= unique_df.loc[unique_df['vocab_compliant']]\n",
    "print(f\"{len(unique_df) - len(filtered_df)} molecules removed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split dataset to create training, validation and test sets\n",
    "\n",
    "Finally, we split the dataset into training, validation, and test sets with a ratio of 7:1:2, respectively. These sets are then saved as CSV files in designated subdirectories (`train`, `val` and `test`) as required for model training. Other dataset variables e.g. columns, headers, dataset_path are specified in the config used for training, see example config files in `examples/conf/molecule` for more information.\n",
    "We use a scaffold split here to make the test set more difficult, as it contains molecules with different structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_splits(data: DataLoader, filtered_df: pd.DataFrame, method: Literal['scaffold', 'random'], seed: int, frac: list) -> dict:\n",
    "    \"\"\"Splits the data into train, validation and test sets by the specified method, according to the specified fractions \n",
    "    and returns a dictionary of dataframes\n",
    "    \"\"\"\n",
    "    # Update data object with filtered molecules\n",
    "    data.entity1 = filtered_df.canonical_smiles\n",
    "    data.entity1_idx = filtered_df.Drug_ID\n",
    "    data.y = filtered_df.Y\n",
    "    print(f\"Generating train/valid/test sets according to {method} strategy in the following fractions of the total dataset: {frac}\")\n",
    "    return data.get_split(method=method, seed=seed, frac=frac)\n",
    "    \n",
    "\n",
    "splits = generate_splits(data, filtered_df, method='scaffold', seed=42, frac= [0.7,0.1,0.2])\n",
    "directory_mapping = {\"valid\":\"val\"}\n",
    "\n",
    "print(\"Writing sets to file...\")\n",
    "for molecule_set in splits.keys():\n",
    "    set_name = directory_mapping.get(molecule_set, molecule_set)\n",
    "    splits[molecule_set].rename(columns= {\"Drug\":\"smiles\"}, inplace =True)\n",
    "    output_dir = os.path.join(bionemo_home, f\"data/processed/{task}/{set_name}\")\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    outfilename = os.path.join(output_dir, f\"{set_name}_set.csv\")\n",
    "    splits[molecule_set].to_csv(outfilename, index=False)\n",
    "    print(f\"Saved {set_name} set molecules to {outfilename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pretrain MolMIM from scratch \n",
    "\n",
    "Here we will run pretraining from scratch using the default config \"pretrain_xsmall_canonicalized.yaml\". Other example configs can be found in directory `examples/molecule/molmim/conf`. We will override some config arguments at runtime using Hydra. \n",
    "\n",
    "The column containing the SMILES strings in the input datasets is specified with the argument `model.data.data_impl_kwargs.csv_mmap.data_col`. We specify the directory containing our newly created `train`, `val` and `test` subdirectories using the argument `model.data.dataset_path` and the names of our input files using argument `model.data.dataset.train` etc.\n",
    "\n",
    "Note, index files are written to the file specified in `model.data.index_mapping_dir`, if index files here are present they will be read rather than written. Therefore, we here clear the index files first to avoid unintentional errors which could occur if changing model or dataset params in this notebook playground.\n",
    "For the same reason we set `exp_manager.resume_if_exists` to be `False`, otherwise if training was interrupted and then restarted, training will continue from the last saved checkpoint, and errors could occur if model params in the config had been changed.\n",
    "\n",
    "We will also reduce the number of training steps (`trainer.max_steps`) and correspondingly the step interval for checking the validation set (`trainer.val_check_interval`) just for the purpose of this demonstration.\n",
    "\n",
    "Optional: Add arguments for logging with Weights and Biases and login when prompted \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf data/data_index\n",
    "! cd {bionemo_home} && python examples/molecule/molmim/pretrain.py \\\n",
    "    do_training=True \\\n",
    "    ++model.data.dataset_path=\"data/processed/{task}/\" \\\n",
    "    ++model.data.dataset.train=\"train_set\" \\\n",
    "    ++model.data.dataset.val=\"val_set\" \\\n",
    "    ++model.data.dataset.test=\"test_set\" \\\n",
    "    ++model.data.index_mapping_dir=\"data/data_index/\" \\\n",
    "    ++model.data.data_impl_kwargs.csv_mmap.data_col=1 \\\n",
    "    ++model.dwnstr_task_validation.enabled=False \\\n",
    "    ++model.global_batch_size=null \\\n",
    "    ++trainer.devices=1 \\\n",
    "    ++trainer.accelerator='gpu' \\\n",
    "    ++trainer.max_steps=200 \\\n",
    "    ++trainer.val_check_interval=100 \\\n",
    "    ++exp_manager.create_wandb_logger=False \\\n",
    "    ++exp_manager.resume_if_exists=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Continue training on an existing model checkpoint\n",
    "\n",
    "To do this, we run the `pretrain.py` script but specify the `++exp_manager.resume_if_exists=True` argument. We will use the model we just trained above and we will override some config arguments at runtime using Hydra. Specifically, we will increase the max_steps argument to `400`, which will train an additional 200 steps on top of the 200 that were taken in the initial training run.\n",
    "\n",
    "Note: ensure the config specified matches the existing model to be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"/result/nemo_experiments/MolMIM/MolMIM-xsmall_pretraining/checkpoints/MolMIM.nemo\"\n",
    "\n",
    "! rm -rf data/data_index\n",
    "! cd {bionemo_home} && python examples/molecule/molmim/pretrain.py \\\n",
    "    do_training=True \\\n",
    "    ++model.data.dataset_path=\"data/processed/{task}/\" \\\n",
    "    ++model.data.dataset.train=\"train_set\" \\\n",
    "    ++model.data.dataset.val=\"val_set\" \\\n",
    "    ++model.data.dataset.test=\"test_set\" \\\n",
    "    ++model.data.index_mapping_dir=\"data/data_index/\" \\\n",
    "    ++model.data.data_impl_kwargs.csv_mmap.data_col=1 \\\n",
    "    ++model.dwnstr_task_validation.enabled=False \\\n",
    "    ++model.global_batch_size=null \\\n",
    "    ++trainer.devices=1 \\\n",
    "    ++trainer.accelerator='gpu' \\\n",
    "    ++trainer.max_steps=400 \\\n",
    "    ++trainer.val_check_interval=100 \\\n",
    "    ++exp_manager.create_wandb_logger=False \\\n",
    "    ++exp_manager.resume_if_exists=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
