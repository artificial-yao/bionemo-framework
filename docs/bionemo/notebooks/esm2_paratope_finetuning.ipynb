{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain from Scratch, Continue Training from an Existing Checkpoint, and Fine-tune ESM-2nv on Custom Data\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> This notebook was tested on a single A1000 GPU and is compatible with BioNeMo Framework v1.6, v1.7 and v1.8 with an expected runtime of approximately 2 hours for ESM-2nv 650M model. This notebook is specific to the ESM-2nv model only. </div>\n",
    "\n",
    "### Demo Objectives\n",
    "1. **Continue Training from a Model Checkpoint**\n",
    "   - **Objective:** Utilize ESM-2nv models for predicting antibody function with an additional prediction head..\n",
    "   - **Steps:** Collect the data, and use existing downstream prediction head training scripts in BioNeMo for token-level classification.\n",
    "\n",
    "2. **Downstream Head Fine-tuning**\n",
    "   - **Objective:** Fine-tune ESM-2nv for predicting antibody function with an additional prediction head.\n",
    "   - **Steps:** Collect the data, and use existing downstream prediction head training scripts in BioNeMo for token-level classification.\n",
    "\n",
    "3. **Full Parameter Fine-tuning on Antibody Sequences**\n",
    "   - **Objective:** Fine-tune an ESM-2nv foundation model and head on antibody sequences to enhance recognition of specific sequence patterns.\n",
    "   - **Steps:** Prepare dataset, and fine-tune ESM-2nv.\n",
    "\n",
    "4. **Low-Rank Adaptation (LoRA) Fine-tuning**\n",
    "   - **Objective:** Apply LoRA to ESM-2nv for antibody sequences to improve efficiency and robustness.\n",
    "   - **Steps:** Integrate LoRA adapters, and fine-tune adapters while freezing core weights.\n",
    "\n",
    "For this purpose, we will use data available from the [Therapeutic Data Commons](https://tdcommons.ai/) for the prediction of amino acid binding in antibody sequences.\n",
    "### Setup\n",
    "\n",
    "Ensure that you have read through the [Getting Started](../index.md) section, can run the BioNeMo Framework Docker container, and have configured the NGC Command Line Interface (CLI) within the container. It is assumed that this notebook is being executed from within the container.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> Some of the cells below generate long text output.  We're using <pre>%%capture --no-display --no-stderr cell_output</pre> to suppress this output.  Comment or delete this line in the cells below to restore full output.</div>\n",
    "\n",
    "### Import and install all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "! pip install PyTDC\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Importing libraries to download and split datasets from the Therapeutic Data Commons https://tdcommons.ai/\n",
    "from tdc.single_pred import Paratope\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bionemo_home = \"/workspace/bionemo\"\n",
    "os.environ['BIONEMO_HOME'] = bionemo_home\n",
    "os.chdir(bionemo_home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Download and Preprocessing\n",
    "**Dataset Overview**: This dataset focuses on paratope prediction, which involves identifying the active binding regions within an antibody. It compiles sequences from SAbDab, encompassing both the heavy and light chains of the antibody.\n",
    "\n",
    "**Objective**: The task involves classifying at the token level. For a given sequence of amino acids, the goal is to identify the specific amino acid tokens that are active in binding. In this context, `X` represents the amino acid sequence, while `Y` denotes the indices of active binding positions within `X`.\n",
    "\n",
    "**Dataset Details**: The dataset comprises sequences from 1,023 antibody chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing raw SAbDab_Liberis dataset from TD Commons.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antibody_ID</th>\n",
       "      <th>Antibody</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2hh0_H</td>\n",
       "      <td>LEQSGAELVKPGASVKLSCTASGFNIEDSYIHWVKQRPEQGLEWIG...</td>\n",
       "      <td>[49, 80, 81, 82, 101]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1u8q_B</td>\n",
       "      <td>ITLKESGPPLVKPTQTLTLTCSFSGFSLSDFGVGVGWIRQPPGKAL...</td>\n",
       "      <td>[30, 31, 53, 83, 84, 85, 104, 105, 106, 107, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4ydl_H</td>\n",
       "      <td>EVRLVQSGNQVRKPGASVRISCEASGYKFIDHFIHWVRQVPGHGLE...</td>\n",
       "      <td>[52, 67, 68, 85, 86, 87, 106, 107]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4ydl_L</td>\n",
       "      <td>EIVLTQSPGTLSLSPGETATLSCRTSQGILSNQLAWHQQRRGQPPR...</td>\n",
       "      <td>[30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1mhp_X</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGFTFSRYTMSWVRQAPGKGLE...</td>\n",
       "      <td>[52, 82, 83, 84, 103, 104]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Antibody_ID                                           Antibody  \\\n",
       "0      2hh0_H  LEQSGAELVKPGASVKLSCTASGFNIEDSYIHWVKQRPEQGLEWIG...   \n",
       "1      1u8q_B  ITLKESGPPLVKPTQTLTLTCSFSGFSLSDFGVGVGWIRQPPGKAL...   \n",
       "2      4ydl_H  EVRLVQSGNQVRKPGASVRISCEASGYKFIDHFIHWVRQVPGHGLE...   \n",
       "3      4ydl_L  EIVLTQSPGTLSLSPGETATLSCRTSQGILSNQLAWHQQRRGQPPR...   \n",
       "4      1mhp_X  EVQLVESGGGLVQPGGSLRLSCAASGFTFSRYTMSWVRQAPGKGLE...   \n",
       "\n",
       "                                                   Y  \n",
       "0                              [49, 80, 81, 82, 101]  \n",
       "1  [30, 31, 53, 83, 84, 85, 104, 105, 106, 107, 1...  \n",
       "2                 [52, 67, 68, 85, 86, 87, 106, 107]  \n",
       "3                                               [30]  \n",
       "4                         [52, 82, 83, 84, 103, 104]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify dataset for download\n",
    "dataset_name = 'SAbDab_Liberis'\n",
    "print(f\"Preparing raw {dataset_name} dataset from TD Commons.\")\n",
    "data = Paratope(name = dataset_name)\n",
    "data_df = data.get_data()\n",
    "splits = data.get_split()\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each antibody sequence (`Antibody`) is a string of amino acids, where the order and composition determine its function and specificity. Within each antibody sequence, specific positions (`Y`) are crucial for its function and denote their belonging to the paratope of the antibody. Here, we define a function (`encode_sequence`) to encode these specific positions in the antibody sequence by initializing a sequence with placeholders (`N` for non-paratope positions) and marking the positions of interest with a label `P`, denoting amino acids that belong to the paratope. The dataset is then divided into subsets (`train`, `val`, `test`), and each subset undergoes the encoding strategy.\n",
    "\n",
    "For ESM-2 to be trained on custom sequences, we will also need to create FASTA files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded sequences saved as x000.csv in /workspace/bionemo/data/processed/SAbDab_Liberis/paratope/train\n",
      "Encoded sequences saved as x000.fasta in /workspace/bionemo/data/processed/SAbDab_Liberis/paratope/train\n",
      "Encoded sequences saved as x000.csv in /workspace/bionemo/data/processed/SAbDab_Liberis/paratope/val\n",
      "Encoded sequences saved as x000.fasta in /workspace/bionemo/data/processed/SAbDab_Liberis/paratope/val\n",
      "Encoded sequences saved as x000.csv in /workspace/bionemo/data/processed/SAbDab_Liberis/paratope/test\n",
      "Encoded sequences saved as x000.fasta in /workspace/bionemo/data/processed/SAbDab_Liberis/paratope/test\n"
     ]
    }
   ],
   "source": [
    "base_data_dir = os.path.join(bionemo_home, 'data')\n",
    "task_name = \"paratope\"\n",
    "!mkdir -p {base_data_dir}/processed/{dataset_name}\n",
    "\n",
    "SAbDab_dir = os.path.join(base_data_dir, 'processed', dataset_name)\n",
    "\n",
    "def encode_sequence(row: pd.Series) -> str:\n",
    "    sequence = list('N' * len(row['Antibody']))  # Create a list of 'N's the same length as the sequence\n",
    "    # Check if row['Y'] is a string that needs to be evaluated\n",
    "    if isinstance(row['Y'], str):\n",
    "        positions = eval(row['Y'])  # Convert string representation of list to actual list\n",
    "    else:\n",
    "        positions = row['Y']  # Assume row['Y'] is already in the correct format (e.g., a list)\n",
    "    for pos in positions:\n",
    "        adj_pos = pos - 1  # Adjust the position to 0-based indexing\n",
    "        sequence[adj_pos] = 'P'  # Encode the position as 'P'\n",
    "    return ''.join(sequence)  # Convert the list back to a string\n",
    "\n",
    "# List of split names, assuming they are 'train', 'valid', and 'test'\n",
    "# Update 'valid' to 'val' for the folder name\n",
    "split_names = ['train', 'val', 'test']\n",
    "\n",
    "for split_name in split_names:\n",
    "    # Adjust the key for accessing the validation split if necessary\n",
    "    split_key = 'valid' if split_name == 'val' else split_name\n",
    "    # Construct the file path\n",
    "    df = splits[split_key]\n",
    "    # Apply the function to each row\n",
    "    df['Encoded'] = df.apply(encode_sequence, axis=1)\n",
    "\n",
    "    # Adjust the directory structure for saving, now including the task_name\n",
    "    task_specific_dir = os.path.join(SAbDab_dir, task_name, split_name)\n",
    "    os.makedirs(task_specific_dir, exist_ok=True)  # Ensure the directory exists\n",
    "    df = df[['Antibody', 'Encoded']]  # Reorder the columns\n",
    "    # Save the modified DataFrame to the new path\n",
    "    df.to_csv(os.path.join(task_specific_dir, f\"x000.csv\"), index=False)\n",
    "    print(f\"Encoded sequences saved as x000.csv in {task_specific_dir}\")\n",
    "    \n",
    "    # Save as FASTA\n",
    "    fasta_path = os.path.join(task_specific_dir, f\"x000.fasta\")\n",
    "    with open(fasta_path, 'w') as fasta_file:\n",
    "        for index, row in df.iterrows():\n",
    "            fasta_file.write(f\">Sequence_{index}\\n{row['Antibody']}\\n\")\n",
    "    print(f\"Encoded sequences saved as x000.fasta in {task_specific_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antibody</th>\n",
       "      <th>Encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LEQSGAELVKPGASVKLSCTASGFNIEDSYIHWVKQRPEQGLEWIG...</td>\n",
       "      <td>NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ITLKESGPPLVKPTQTLTLTCSFSGFSLSDFGVGVGWIRQPPGKAL...</td>\n",
       "      <td>NNNNNNNNNNNNNNNNNNNNNNNNNNNNNPPNNNNNNNNNNNNNNN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EVRLVQSGNQVRKPGASVRISCEASGYKFIDHFIHWVRQVPGHGLE...</td>\n",
       "      <td>NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EVQLSESGGGFVKPGGSLRLSCEASGFTFNNYAMGWVRQAPGKGLE...</td>\n",
       "      <td>NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QVQLVQPGTAMKSLGSSLTITCRVSGDDLGSFHFGTYFMIWVRQAP...</td>\n",
       "      <td>NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNPPPPPNNNNNNNNNNN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Antibody  \\\n",
       "0  LEQSGAELVKPGASVKLSCTASGFNIEDSYIHWVKQRPEQGLEWIG...   \n",
       "1  ITLKESGPPLVKPTQTLTLTCSFSGFSLSDFGVGVGWIRQPPGKAL...   \n",
       "2  EVRLVQSGNQVRKPGASVRISCEASGYKFIDHFIHWVRQVPGHGLE...   \n",
       "3  EVQLSESGGGFVKPGGSLRLSCEASGFTFNNYAMGWVRQAPGKGLE...   \n",
       "4  QVQLVQPGTAMKSLGSSLTITCRVSGDDLGSFHFGTYFMIWVRQAP...   \n",
       "\n",
       "                                             Encoded  \n",
       "0  NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN...  \n",
       "1  NNNNNNNNNNNNNNNNNNNNNNNNNNNNNPPNNNNNNNNNNNNNNN...  \n",
       "2  NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN...  \n",
       "3  NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN...  \n",
       "4  NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNPPPPPNNNNNNNNNNN...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_df = pd.read_csv(os.path.join(SAbDab_dir, task_name, 'train', 'x000.csv'))\n",
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Model Checkpoints\n",
    "\n",
    "The following code will download the pretrained model `esmn2nv_650M_converted.nemo` from the NGC registry.\n",
    "\n",
    "In BioNeMo FW, there are numerous ESM models available, including ESM-1nv, ESM-2nv 8M with randomly initialized weights, ESM-2nv fine-tuned for secondary structure downstream prediction tasks with LoRA, ESM-2nv 650M, and ESM-2nv 3B. We also have a configuration file for training ESM-2nv 15B available at `examples/protein/esm2nv/conf/pretrain_esm2_15B.yaml`, if needed.\n",
    "\n",
    "For demo purposes, we have chosen to showcase the ESM-2nv 650M model. For more details on the [ESM-1nv](https://docs.nvidia.com/bionemo-framework/latest/models/esm1-nv.html) or [ESM-2nv](https://docs.nvidia.com/bionemo-framework/latest/models/esm2-nv.html), consult the corresponding model cards. To find the model names and checkpoint names please refer to the `artifacts_paths.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NGC CLI API KEY and ORG for the model download\n",
    "# If these variables are not already set in the container, uncomment below\n",
    "# to define and set with your API KEY and ORG\n",
    "# api_key = <YOUR_API_KEY>\n",
    "# ngc_cli_org = <YOUR_ORG>\n",
    "# Update the environment variable\n",
    "# os.environ['NGC_CLI_API_KEY'] = api_key\n",
    "# os.environ['NGC_CLI_ORG'] = ngc_cli_org\n",
    "\n",
    "# Set variables and paths for model and checkpoint\n",
    "model_name = \"esm2nv\" \n",
    "model_version = \"esm2nv_650m\" \n",
    "actual_checkpoint_name = \"esm2nv_650M_converted.nemo\"\n",
    "model_path = os.path.join(bionemo_home, 'models')\n",
    "checkpoint_path = os.path.join(model_path, actual_checkpoint_name)\n",
    "os.environ['MODEL_PATH'] = model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    !cd /workspace/bionemo && \\\n",
    "    python download_artifacts.py --model_dir models --models {model_version}\n",
    "else:\n",
    "    print(f\"Model {model_version} already exists at {model_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up paths to the data used for model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/workspace/bionemo/data/processed/SAbDab_Liberis/paratope_custom_dataset’: File exists\n"
     ]
    }
   ],
   "source": [
    "config_dir = os.path.join(bionemo_home, f'examples/protein/{model_name}/conf')\n",
    "train_fasta = os.path.join(SAbDab_dir, f'{task_name}/train/x000.fasta')\n",
    "val_fasta = os.path.join(SAbDab_dir, f'{task_name}/val/x000.fasta')\n",
    "test_fasta = os.path.join(SAbDab_dir, f'{task_name}/test/x000.fasta')\n",
    "paratope_dir = os.path.join(SAbDab_dir, 'paratope_custom_dataset')\n",
    "! mkdir {paratope_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Pretraining from Scratch\n",
    "- Performing preprocessing on the data to transform it into a format that can be used by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "! cd {bionemo_home} && python examples/protein/esm2nv/pretrain.py \\\n",
    "  --config-path={config_dir} \\\n",
    "  --config-name=pretrain_esm2_650M \\\n",
    "  ++do_training=False \\\n",
    "  ++do_preprocessing=True \\\n",
    "  ++trainer.devices=1 \\\n",
    "  ++model.data.train.custom_pretraining_fasta_path={train_fasta} \\\n",
    "  ++model.data.val.custom_pretraining_fasta_path={val_fasta} \\\n",
    "  ++model.data.test.custom_pretraining_fasta_path={test_fasta} \\\n",
    "  ++model.data.dataset_path={paratope_dir} \\\n",
    "  ++model.data.train.dataset_path={paratope_dir} \\\n",
    "  ++exp_manager.create_wandb_logger=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrain from scratch\n",
    "\n",
    "This will take approximately 15 minutes on a A1000 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "! cd {bionemo_home} && python examples/protein/esm2nv/pretrain.py \\\n",
    "    --config-path={config_dir} \\\n",
    "    --config-name=pretrain_esm2_650M \\\n",
    "    name={model_name}_from_scratch_antibodies \\\n",
    "    ++do_training=True \\\n",
    "    ++trainer.devices=1 \\\n",
    "    ++trainer.max_steps=1 \\\n",
    "    ++trainer.val_check_interval=1 \\\n",
    "    ++model.data.train.custom_pretraining_fasta_path={train_fasta} \\\n",
    "    ++model.data.val.custom_pretraining_fasta_path={val_fasta} \\\n",
    "    ++model.data.test.custom_pretraining_fasta_path={test_fasta} \\\n",
    "    ++model.data.dataset_path={paratope_dir} \\\n",
    "    ++model.data.train.dataset_path={paratope_dir} \\\n",
    "    ++model.micro_batch_size=1 \\\n",
    "    ++exp_manager.create_wandb_logger=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue Pretraining, Add a Downstream Head, Perform Full Parameter Fine-Tuning for ESM-2nv on Antibody Sequences\n",
    "\n",
    "#### 1. Continue training from a model checkpoint\n",
    "\n",
    "In BioNeMo, you can easily continue training ESM-2nv on antibody sequences from a `.nemo` checkpoint\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>IMPORTANT</b>: For demonstration purposes, the `max_steps` and `val_check_interval` parameters in the fine-tuning process have been adjusted to lower values. </div>\n",
    "\n",
    "To continue the pretraining of the foundation model, use the `pretrain.py` script and set `exp_manager.resume_if_exists=True` to load the model weights, maintain metadata from the previous run (e.g. max_steps) and it picks up from the learning rate at the end of the previous run from the existing `esm2nv_650M_converted.nemo` checkpoint file. You can replace this file with another, but ensure to select the correct config file relative to the model of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "! cd {bionemo_home} && python /workspace/bionemo/examples/protein/esm2nv/pretrain.py \\\n",
    "    --config-path={config_dir} \\\n",
    "    --config-name=pretrain_esm2_650M \\\n",
    "    name={model_name}_antibodies_continued \\\n",
    "    do_training=True \\\n",
    "    ++trainer.devices=1 \\\n",
    "    ++trainer.max_steps=1 \\\n",
    "    ++trainer.val_check_interval=1 \\\n",
    "    ++model.data.train.custom_pretraining_fasta_path={train_fasta} \\\n",
    "    ++model.data.val.custom_pretraining_fasta_path={val_fasta} \\\n",
    "    ++model.data.test.custom_pretraining_fasta_path={test_fasta} \\\n",
    "    ++model.data.dataset_path={paratope_dir} \\\n",
    "    ++model.data.train.dataset_path={paratope_dir} \\\n",
    "    ++model.micro_batch_size=1 \\\n",
    "    ++exp_manager.create_wandb_logger=false \\\n",
    "    ++exp_manager.resume_if_exists=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Downstream Head Fine-Tuning\n",
    "First, note that we are not using the `pretrain.py` script but rather the `downstream_flip.py` script. This script was originally created for downstream fine-tuning on the FLIP dataset. In addition to this Python script, we will use a `yaml` file that already exists in BioNeMo for the `token-level-classification` task, specifically named `downstream_flip_sec_str`. We will override the configurations using Hydra. In particular, we do not want to perform training; instead, we want to add a prediction head, which in this case will be a `Conv2D` head for `token-level-classification`.\n",
    "\n",
    "We will need to adjust the `dwnstr_task_validation` configurations as well as the data used by the `model`. In addition to setting the correct data paths, it is necessary to specify the number of classes we are predicting under `target_sizes` as a list, as these will be used by the CNN. You can also provide mask columns; otherwise, set them to `null` as a list. The `target_column` should be the column in the dataframe where we have the labels, in this case, sequences labeled with `N` and `P` characters. Along with the labels, we need to specify the sequence column as well.\n",
    "\n",
    "Importantly, we need to set the encoder path to `esm2nv_650M_converted.nemo`. By default, the `encoder_frozen` parameter is set to `True`, meaning that the foundation model weights are fixed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = os.path.join(SAbDab_dir, f'{task_name}/train/x000.csv')\n",
    "val_data = os.path.join(SAbDab_dir, f'{task_name}/val/x000.csv')\n",
    "test_data = os.path.join(SAbDab_dir, f'{task_name}/test/x000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "! cd {bionemo_home} && python examples/protein/downstream/downstream_flip.py \\\n",
    "    --config-path={config_dir} \\\n",
    "    --config-name=downstream_flip_sec_str \\\n",
    "    name={model_name}_with_head \\\n",
    "    do_training=True \\\n",
    "    do_testing=True \\\n",
    "    ++data.dataset_path={SAbDab_dir} \\\n",
    "    ++trainer.devices=1 \\\n",
    "    ++trainer.max_steps=1 \\\n",
    "    ++trainer.val_check_interval=1 \\\n",
    "    ++model.data.dataset.train={train_data} \\\n",
    "    ++model.data.dataset.val={val_data} \\\n",
    "    ++model.data.dataset.test={test_data} \\\n",
    "    ++model.data.target_column=['Encoded'] \\\n",
    "    ++model.data.sequence_column=\"Antibody\" \\\n",
    "    ++model.data.target_sizes=[2] \\\n",
    "    ++model.data.mask_column=[null] \\\n",
    "    ++model.micro_batch_size=1 \\\n",
    "    ++model.data.task_name={task_name} \\\n",
    "    ++model.restore_encoder_path={checkpoint_path} \\\n",
    "    ++model.dwnstr_task_validation.dataset.dataset_path={SAbDab_dir} \\\n",
    "    ++model.data.preprocessed_data_path={SAbDab_dir} \\\n",
    "    ++exp_manager.create_wandb_logger=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Full Parameter Fine-Tuning\n",
    "Fine-tuning the foundation model will require us to use the `downstream_flip.py` script and set `restore_encoder_path` to load the model weights from the existing checkpoint file. Also, ensure that the encoder weights are not frozen by setting `model.encoder_frozen=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "! cd {bionemo_home} && python examples/protein/downstream/downstream_flip.py \\\n",
    "    --config-path={config_dir} \\\n",
    "    --config-name=downstream_flip_sec_str \\\n",
    "    name={model_name}_full_fine_tuning \\\n",
    "    do_training=True \\\n",
    "    do_testing=True \\\n",
    "    ++data.dataset_path={SAbDab_dir} \\\n",
    "    ++trainer.devices=1 \\\n",
    "    ++trainer.max_steps=1 \\\n",
    "    ++trainer.val_check_interval=1 \\\n",
    "    ++model.data.dataset.train={train_data} \\\n",
    "    ++model.data.dataset.val={val_data} \\\n",
    "    ++model.data.dataset.test={test_data} \\\n",
    "    ++model.data.target_column=['Encoded'] \\\n",
    "    ++model.data.sequence_column=\"Antibody\" \\\n",
    "    ++model.data.target_sizes=[2] \\\n",
    "    ++model.data.mask_column=[null] \\\n",
    "    ++model.micro_batch_size=1 \\\n",
    "    ++model.data.task_name={task_name} \\\n",
    "    ++model.restore_encoder_path={checkpoint_path} \\\n",
    "    ++model.dwnstr_task_validation.dataset.dataset_path={SAbDab_dir} \\\n",
    "    ++model.data.preprocessed_data_path={SAbDab_dir} \\\n",
    "    ++exp_manager.create_wandb_logger=false \\\n",
    "    ++model.encoder_frozen=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Low-Rank Adaptation (LoRA) fine-tuning\n",
    "\n",
    "A few notable changes in the `downstream_sec_str_LORA.yaml` file are:\n",
    "\n",
    "- `model.encoder_frozen`= `False`. Set to `False` when using PEFT.\n",
    "\n",
    "- `model.peft.enabled`= `True`. Set to `True` to enable PEFT.\n",
    "\n",
    "- `model.peft.lora_tuning.adapter_dim`: Allows setting different values for the rank used in matrix decomposition. This hyperparameter helps maximize performance on your data, as it determines the number of trainable parameters.\n",
    "\n",
    "- `model.peft.lora_tuning.layer_selection`: Selects the layers in which to add LoRA adapters. For example, `[1,12]` will add LoRA to layer 1 (lowest) and layer 12. `null` will apply adapters to all layers.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> LoRA is currently not supported for esm-1nv</div>\n",
    "\n",
    "Following [these instructions](/bionemo/docs/bionemo/lora-finetuning-esm2.md) and reimplementing the `ESM2nvLoRAModel` class in the `bionemo/model/protein/esm1nv/esm1nv_model.py` script for ESM-1, you can perform LoRA.\n",
    "\n",
    "For more details about LoRA please see [this](./esm2_FLIP_finetuning.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "! cd {bionemo_home} && python examples/protein/downstream/downstream_flip.py \\\n",
    "    --config-path={config_dir} \\\n",
    "    --config-name=downstream_sec_str_LORA \\\n",
    "    name={model_name}_LORA \\\n",
    "    do_training=True \\\n",
    "    do_testing=True \\\n",
    "    ++data.dataset_path={SAbDab_dir} \\\n",
    "    ++trainer.devices=1 \\\n",
    "    ++trainer.max_steps=1 \\\n",
    "    ++trainer.max_epochs=1 \\\n",
    "    ++trainer.val_check_interval=1 \\\n",
    "    ++model.encoder_frozen=False \\\n",
    "    ++model.data.task_name={task_name} \\\n",
    "    ++model.restore_encoder_path={checkpoint_path} \\\n",
    "    ++model.data.preprocessed_data_path={SAbDab_dir} \\\n",
    "    ++model.data.dataset.train={train_data} \\\n",
    "    ++model.data.dataset.val={val_data} \\\n",
    "    ++model.data.dataset.test={test_data} \\\n",
    "    ++model.data.target_column=['Encoded'] \\\n",
    "    ++model.data.sequence_column=\"Antibody\" \\\n",
    "    ++model.data.target_sizes=[2] \\\n",
    "    ++model.data.mask_column=[null] \\\n",
    "    ++model.dwnstr_task_validation.dataset.target_column=['Encoded'] \\\n",
    "    ++model.dwnstr_task_validation.dataset.sequence_column=\"Antibody\" \\\n",
    "    ++model.dwnstr_task_validation.dataset.target_sizes=[2] \\\n",
    "    ++model.dwnstr_task_validation.data_impl_kwargs.csv_mmap.data_col=1 \\\n",
    "    ++model.dwnstr_task_validation.dataset.mask_column=[null] \\\n",
    "    ++model.dwnstr_task_validation.dataset.dataset_path={SAbDab_dir} \\\n",
    "    ++exp_manager.create_wandb_logger=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we explored how to continue training ESM-2nv, add a downstream head, and perform full-parameter fine-tuning (both the foundation model and the head) for a token-level classification task on antibody sequences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
