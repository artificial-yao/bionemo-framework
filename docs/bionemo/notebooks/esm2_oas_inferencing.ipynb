{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing inference on OAS sequences with ESM-2nv\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> This notebook was tested on a single A1000 GPU and is compatible with BioNeMo Framework v1.6, v1.7, and v1.8 with an expected runtime of less than one 1 hour. </div>\n",
    "\n",
    "### Demo Objectives:\n",
    "\n",
    "1. Learn how to bring your own dataset for ESM-2nv inference.\n",
    "2. Load a pretrained ESM-2nv model and perform inference on the prepared input in the previous step.\n",
    "\n",
    "Relevance: \n",
    "\n",
    "Antibodies are among the most successful therapeutics in clinical trials and on the market. They have demonstrated high efficacy and specificity in treating a variety of diseases, contributing to the pharmaceutical market. As of recent years, antibodies have become a dominant class of bio-pharmaceuticals, with several blockbuster drugs generating substantial revenue. For instance, monoclonal antibodies used in oncology, autoimmune diseases, and infectious diseases have achieved widespread clinical success and market penetration.\n",
    "\n",
    "Their success is reflected in their ability to specifically target disease-causing agents or cells, reducing side effects compared to traditional treatments. Market reports consistently highlight antibodies as a leading category in bio-pharmaceuticals, underscoring their pivotal role in modern medicine's therapeutic landscape.\n",
    "\n",
    "We will use ESM-2nv to create embeddings of heavy chain variable domain (VHs) sequences of antibodies found in the [OAS database](https://opig.stats.ox.ac.uk/webapps/oas/).\n",
    "\n",
    "\n",
    "### Setup\n",
    "\n",
    "Ensure that you have read through the [Getting Started](../index.md) section, can run the BioNeMo Framework Docker container, and have configured the NGC Command Line Interface (CLI) within the container. It is assumed that this notebook is being executed from within the container.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> Some of the cells below generate long text output.  We're using <pre>%%capture --no-display --no-stderr cell_output</pre> to suppress this output.  Comment or delete this line in the cells below to restore full output.</div>\n",
    "\n",
    "**You can use this notebook for both ESM-2nv and ESM-1nv by making minor code changes.**\n",
    "\n",
    "### Import and install all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import urllib.request\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bionemo_home = \"/workspace/bionemo\"\n",
    "os.environ['BIONEMO_HOME'] = bionemo_home\n",
    "os.chdir(bionemo_home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Model Checkpoints\n",
    "\n",
    "The following code will download the pretrained model `esmn2nv_650M_converted.nemo` from the NGC registry.\n",
    "\n",
    "In BioNeMo FW, there are numerous ESM models available, including ESM-1nv, ESM-2nv 8M with randomly initialized weights, ESM-2nv fine-tuned to secondary structure downstream prediction tasks with LoRA, ESM-2nv 650M, and ESM-2nv 3B. We also have a configuration file for training ESM-2nv 15B available at `examples/protein/esm2nv/conf/pretrain_esm2_15B.yaml` if needed.\n",
    "\n",
    "For demo purposes, we have chosen to showcase the ESM-2nv 650M model. For more details on the [ESM-1nv](https://docs.nvidia.com/bionemo-framework/latest/models/esm1-nv.html) or [ESM-2nv](https://docs.nvidia.com/bionemo-framework/latest/models/esm2-nv.html), consult the corresponding model cards. To find the model names and checkpoint names, please see the `artifacts_paths.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NGC CLI API KEY and ORG for the model download\n",
    "# If these variables are not already set in the container, uncomment below\n",
    "# to define and set with your API KEY and ORG\n",
    "# api_key = <YOUR_API_KEY>\n",
    "# ngc_cli_org = <YOUR_ORG>\n",
    "# Update the environment variable\n",
    "# os.environ['NGC_CLI_API_KEY'] = api_key\n",
    "# os.environ['NGC_CLI_ORG'] = ngc_cli_org\n",
    "\n",
    "# Set variables and paths for model and checkpoint\n",
    "model_name = \"esm2nv\" # for esm1nv change this to \"esm1nv\"\n",
    "model_version = \"esm2nv_650m\" # for esm1nv change this to \"esm1nv\"\n",
    "actual_checkpoint_name = \"esm2nv_650M_converted.nemo\" # for esm1nv change this to \"esm1nv_converted.nemo\"\n",
    "model_path = os.path.join(bionemo_home, 'models')\n",
    "checkpoint_path = os.path.join(model_path, actual_checkpoint_name)\n",
    "os.environ['MODEL_PATH'] = model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    !cd /workspace/bionemo && \\\n",
    "    python download_artifacts.py --model_dir models --models {model_version}\n",
    "else:\n",
    "    print(f\"Model {model_version} already exists at {model_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation\n",
    "\n",
    "Here we will download the dataset and unzip it. The data was sourced from the [OAS database](https://opig.stats.ox.ac.uk/webapps/oas/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_links = [\n",
    "    'https://opig.stats.ox.ac.uk/webapps/ngsdb/paired/Eccles_2020/csv/SRR10358524_paired.csv.gz']\n",
    "\n",
    "base_data_dir = os.path.join(bionemo_home, 'data', 'OAS_paired')\n",
    "if not os.path.exists(base_data_dir):\n",
    "    os.makedirs(base_data_dir)\n",
    "\n",
    "for file in data_links:\n",
    "    data_file = os.path.join(base_data_dir, os.path.basename(file))\n",
    "    if not os.path.isfile(data_file):\n",
    "        # Corrected wget command\n",
    "        urllib.request.urlretrieve(file, data_file)\n",
    "\n",
    "    # Unzip the file\n",
    "    try:\n",
    "        with gzip.open(data_file, 'rb') as f_in:\n",
    "            with open(data_file[:-3], 'wb') as f_out:  # Remove .gz extension for the output file\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "    except OSError as e:\n",
    "        print(f\"Error opening the file {data_file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a BioNeMo-Compatible Data Format for Inference\n",
    "\n",
    "Now that we have the raw data, we need to process it to be compatible with BioNeMo's expected input. We will convert the data into a CSV file. As we are not performing training in this demo, we will not need to create the typical `train`, `validation`, and `test` splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns ['sequence_id_heavy', 'sequence_alignment_aa_heavy'] have been selected and saved to /workspace/bionemo/data/OAS_paired/filtered_csv/filtered_data.csv\n"
     ]
    }
   ],
   "source": [
    "def transform_and_save_csv(input_data_path: str, output_data_path: str, columns_to_keep: list) -> None:\n",
    "    \"\"\"\n",
    "    Transforms the input CSV by keeping only specified columns and saves the result to output path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV with only specified columns\n",
    "        df = pd.read_csv(input_data_path, skiprows=[0], usecols=columns_to_keep)\n",
    "        \n",
    "        # Write the filtered data to a new CSV file\n",
    "        df.to_csv(output_data_path, index=False)\n",
    "        \n",
    "        print(f\"Columns {columns_to_keep} have been selected and saved to {output_data_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "data_path = f'{base_data_dir}/SRR10358524_paired.csv'\n",
    "columns_to_keep = ['sequence_id_heavy', 'sequence_alignment_aa_heavy']\n",
    "\n",
    "filtered_data_path = f'{base_data_dir}/filtered_csv'\n",
    "! mkdir -p {filtered_data_path}\n",
    "processed_data_file = f'{filtered_data_path}/filtered_data.csv'\n",
    "transform_and_save_csv(data_path, processed_data_file, columns_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Inference Using the Existing ESM Model\n",
    "To perform inference on the antibody sequences using the infer.py script, ensure that the following requirements are met:\n",
    "\n",
    "1. Create and prepare a designated output directory to store the results.\n",
    "2. Specify a file path within this directory where the embeddings will be saved. This can be in `pkl` or `h5` format.\n",
    "\n",
    "If the output format is `pkl` and the file is saved using the `pickle` module, the predictions (embeddings and/or hidden states) will be serialized, and the file will contain a dictionary where each key is a sequence identifier and the corresponding value is the predicted output for that sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config_path = os.path.join(bionemo_home, f'examples/protein/{model_name}/conf')\n",
    "output_dir = f'{base_data_dir}/filtered_csv/inference_output' # where we want to save the output \n",
    "! mkdir -p {output_dir}\n",
    "inference_results = f'{output_dir}/{model_name}_oas.pkl' # the name of the output file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input file is expected to have an `id` column at index 0 and a `sequence` column at index 1, as specified by default in the `/bionemo/examples/conf/base_infer_config.yaml` config file. Change these parameters to suit your data files in your `.yaml` file or by using Hydra to override the default settings.\n",
    "\n",
    "It is also important to specify which model we want to use for inference by setting `model.downstream_task.restore_path` to the `checkpoint_path` variable.\n",
    "The results will be saved in the `output_dir` specified, and a log folder will be created for the experiment run and the `inference_results` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "! python /workspace/bionemo/bionemo/model/infer.py \\\n",
    "    --config-dir {model_config_path} \\\n",
    "    --config-name infer \\\n",
    "    ++name={model_name}_Inference_OAS \\\n",
    "    ++model.downstream_task.restore_path={checkpoint_path} \\\n",
    "    ++model.data.dataset_path={processed_data_file} \\\n",
    "    ++exp_manager.exp_dir={output_dir} \\\n",
    "    ++model.inference_output_file={inference_results} \\\n",
    "    ++model.data.output_fname={inference_results}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now access the embeddings saved in the `pkl` format as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sequence: QVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYMHWVRQAPGQGLEWMGWINPNSGGTNYAQKFQGRVTMTRDTSISTAYMELSRLRSDDTAVYYCARESQIVVVPAAIEDYYYYGMDVWGQGTTVTVSS\n",
      "The number of features for a single embedded sequence: (1280,)\n",
      "Inspecting the features vector for a sequence: [-0.01926641 -0.04979213 -0.10104819 ... -0.18573356  0.03264603\n",
      "  0.140413  ]\n"
     ]
    }
   ],
   "source": [
    "# Loadup the parameter model predictions\n",
    "with open(inference_results, 'rb') as fd:\n",
    "     infer_results = pkl.load(fd)\n",
    "print(f\"This is a sequence: {infer_results[-1]['sequence']}\") # the sequence that was embedded\n",
    "print(f\"The number of features for a single embedded sequence: {infer_results[-1]['embeddings'].shape}\") # the number of features for a single embedded sequence\n",
    "print(f\"Inspecting the features vector for a sequence: {infer_results[-1]['embeddings']}\") # inspecting the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call to Action\n",
    "\n",
    "In your own time:\n",
    "- Generate embeddings on the UniRef50 data.\n",
    "- Use the embeddings here generated.\n",
    "- Cluster both sets of embeddings (proteins and antibodies) using UMAP and see if you can identify any patterns. You might gain inspiration from the [protein clustering notebook](protein-esm2nv-clustering.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
