{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune ESM-2nv on FLIP Data for Sequence-Level Classification, Regression, Token-Level Classification, and with LoRA Adapters\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "  <b>NOTE:</b> This notebook has been tested on both an A1000 GPU and an A100, and is compatible with BioNeMo Framework versions 1.6, 1.7, and 1.8. The expected runtime is approximately 2 hours on the A1000 and 10 minutes on the A100. Both tests were performed for the esm2nv-650M model.\n",
    "</div>\n",
    "\n",
    "### Demo Objectives\n",
    "\n",
    "**Downstream Head Fine-Tuning**\n",
    "   - **Objective:** Utilize fine-tuned ESM-2nv models for predicting antibody function with an additional prediction head.\n",
    "   - **Steps:** Collect the data using the existing scripts in BioNeMo for preprocessing, \n",
    "               and use the existing downstream prediction head training scripts in BioNeMo for sequence-level classification, sequence-level regression, token-level classification, and with LoRA adapters.\n",
    "\n",
    "For these purposes, we will use the Fitness Landscape Inference for Proteins (FLIP) evaluation dataset. The FLIP datasets are used to evaluate the performance of protein language models on five specific downstream tasks related to proteins. These tasks include secondary structure prediction, conservation analysis, subcellular localization, meltome analysis, and GB1 activity measurement.\n",
    "\n",
    "### Setup\n",
    "\n",
    "Ensure that you have read through the [Getting Started](../index.md) section, can run the BioNeMo Framework Docker container, and have configured the NGC Command Line Interface (CLI) within the container. It is assumed that this notebook is being executed from within the container.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> Some of the cells below generate long text output. We're using <pre>%%capture --no-display --no-stderr cell_output</pre> to suppress this output. Comment or delete this line in the cells below to restore full output.</div>\n",
    "\n",
    "**You can use this notebook for both ESM-2nv and ESM-1nv (except for LoRA) by making minor code changes**.\n",
    "\n",
    "### Import and Install All Required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from bionemo.data import FLIPPreprocess\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bionemo_home = '/workspace/bionemo'\n",
    "bionemo_home = os.environ['BIONEMO_HOME']\n",
    "os.chdir(bionemo_home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Model Checkpoints\n",
    "\n",
    "The following code will download the pretrained model `esmn2nv_650M_converted.nemo` from the NGC registry.\n",
    "\n",
    "In BioNeMo FW, there are numerous ESM models available, including ESM-1nv, ESM-2nv 8M with randomly initialized weights, ESM-2nv fine-tuned to secondary structure downstream prediction tasks with LoRA, ESM-2nv 650M, and ESM-2nv 3B. We also have a configuration file for training ESM-2nv 15B available at `examples/protein/esm2nv/conf/pretrain_esm2_15B.yaml` if needed.\n",
    "\n",
    "For demo purposes, we have chosen to showcase the ESM-2nv 650M model. For more details on the [ESM-1nv](https://docs.nvidia.com/bionemo-framework/latest/models/esm1-nv.html) or [ESM-2nv](https://docs.nvidia.com/bionemo-framework/latest/models/esm2-nv.html), consult the corresponding model cards. To find the model names and checkpoint names please see the `artifacts_paths.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NGC CLI API KEY and ORG for the model download\n",
    "# If these variables are not already set in the container, uncomment below\n",
    "# to define and set with your API KEY and ORG\n",
    "#api_key = <your_api_key>\n",
    "#ngc_cli_org = <ngc_cli_org>\n",
    "# Update the environment variable\n",
    "#os.environ['NGC_CLI_API_KEY'] = api_key\n",
    "#os.environ['NGC_CLI_ORG'] = ngc_cli_org\n",
    "\n",
    "# Set variables and paths for model and checkpoint\n",
    "model_name = \"esm2nv\" # change to esm1 for ESM1\n",
    "model_version = \"esm2nv_650m\" # change to esm1nv for ESM1\n",
    "actual_checkpoint_name = \"esm2nv_650M_converted.nemo\" #  change to esm1nv.nemo for ESM1\n",
    "model_path = os.path.join(bionemo_home, 'models')\n",
    "checkpoint_path = os.path.join(model_path, actual_checkpoint_name)\n",
    "os.environ['MODEL_PATH'] = model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    !cd /workspace/bionemo && \\\n",
    "    python download_artifacts.py --model_dir models --models {model_version}\n",
    "else:\n",
    "    print(f\"Model {model_version} already exists at {model_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the FLIP Data and Preprocess It\n",
    "\n",
    "The code below uses the FLIP preprocessing method to download and preprocess the [public FLIP data](http://data.bioembeddings.com/public/FLIP/) into a BioNeMo-compatible format. It will create a folder `data/FLIP` with subdirectories containing the data.\n",
    "\n",
    "In this demo, we are going to predict various properties of protein sequences:\n",
    "1. The protein's subcellular localization (`scl`).\n",
    "2. The melting temperature of a protein (`meltome`).\n",
    "3. The secondary structure of an amino acid (`secondary_structure`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-08-28 13:27:30 flip_preprocess:114] mixed_soft.fasta downloaded successfully!\n",
      "[NeMo I 2024-08-28 13:27:30 flip_preprocess:237] FLIP data download complete.\n",
      "[NeMo I 2024-08-28 13:27:30 flip_preprocess:239] Processing FLIP dataset.\n",
      "[NeMo I 2024-08-28 13:27:30 flip_preprocess:245] Writing processed dataset files to /workspace/bionemo/data/FLIP/scl...\n",
      "[NeMo I 2024-08-28 13:27:30 flip_preprocess:159] Saving train split...\n",
      "[NeMo I 2024-08-28 13:27:30 flip_preprocess:159] Saving val split...\n",
      "[NeMo I 2024-08-28 13:27:30 flip_preprocess:159] Saving test split...\n",
      "[NeMo I 2024-08-28 13:27:30 flip_preprocess:257] FLIP dataset preprocessing completed\n",
      "[NeMo I 2024-08-28 13:27:32 flip_preprocess:114] mixed_split.fasta downloaded successfully!\n",
      "[NeMo I 2024-08-28 13:27:32 flip_preprocess:237] FLIP data download complete.\n",
      "[NeMo I 2024-08-28 13:27:32 flip_preprocess:239] Processing FLIP dataset.\n",
      "[NeMo I 2024-08-28 13:27:32 flip_preprocess:245] Writing processed dataset files to /workspace/bionemo/data/FLIP/meltome...\n",
      "[NeMo I 2024-08-28 13:27:33 flip_preprocess:159] Saving train split...\n",
      "[NeMo I 2024-08-28 13:27:33 flip_preprocess:159] Saving val split...\n",
      "[NeMo I 2024-08-28 13:27:33 flip_preprocess:159] Saving test split...\n",
      "[NeMo I 2024-08-28 13:27:33 flip_preprocess:257] FLIP dataset preprocessing completed\n",
      "[NeMo I 2024-08-28 13:27:34 flip_preprocess:114] sequences.fasta downloaded successfully!\n",
      "[NeMo I 2024-08-28 13:27:35 flip_preprocess:114] sampled.fasta downloaded successfully!\n",
      "[NeMo I 2024-08-28 13:27:36 flip_preprocess:114] resolved.fasta downloaded successfully!\n",
      "[NeMo I 2024-08-28 13:27:36 flip_preprocess:237] FLIP data download complete.\n",
      "[NeMo I 2024-08-28 13:27:36 flip_preprocess:239] Processing FLIP dataset.\n",
      "[NeMo I 2024-08-28 13:27:36 flip_preprocess:245] Writing processed dataset files to /workspace/bionemo/data/FLIP/secondary_structure...\n",
      "[NeMo I 2024-08-28 13:27:36 flip_preprocess:159] Saving train split...\n",
      "[NeMo I 2024-08-28 13:27:36 flip_preprocess:159] Saving val split...\n",
      "[NeMo I 2024-08-28 13:27:36 flip_preprocess:159] Saving test split...\n",
      "[NeMo I 2024-08-28 13:27:36 flip_preprocess:257] FLIP dataset preprocessing completed\n"
     ]
    }
   ],
   "source": [
    "preprocessor = FLIPPreprocess()\n",
    "for task in [\"scl\", \"meltome\", \"secondary_structure\"]:\n",
    "    task_dir = f'{bionemo_home}/data/FLIP/{task}'\n",
    "    preprocessor.prepare_dataset(task_name=task, output_dir=task_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demo purposes, we will subsample the datasets to enable faster execution of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File has been subsampled and saved: data/FLIP/scl/train/x000.csv\n",
      "File has been subsampled and saved: data/FLIP/meltome/train/x000.csv\n",
      "File has been subsampled and saved: data/FLIP/secondary_structure/train/x000.csv\n",
      "File has been subsampled and saved: data/FLIP/scl/val/x000.csv\n",
      "File has been subsampled and saved: data/FLIP/meltome/val/x000.csv\n",
      "File has been subsampled and saved: data/FLIP/secondary_structure/val/x000.csv\n",
      "File has been subsampled and saved: data/FLIP/scl/test/x000.csv\n",
      "File has been subsampled and saved: data/FLIP/meltome/test/x000.csv\n",
      "File has been subsampled and saved: data/FLIP/secondary_structure/test/x000.csv\n"
     ]
    }
   ],
   "source": [
    "def subsample_data(path: str, subsample_type: str, column: str = None, subset_fraction: float = 0.05, random_seed: int = 0) -> None:\n",
    "    \"\"\"\n",
    "    Subsamples the dataset based on the specified subsample type.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Function to sample a subset of the data by class\n",
    "    def sample_by_class(df, fraction):\n",
    "        return df.groupby(column, group_keys=False).apply(lambda x: x.sample(frac=fraction, random_state=random_seed))\n",
    "    \n",
    "    # Function to sample a subset of the data by continuous variable\n",
    "    def sample_by_continuous(df, fraction):\n",
    "        stratify_bins = pd.qcut(df[column], q=10, duplicates='drop')\n",
    "        return df.groupby(stratify_bins, group_keys=False).apply(lambda x: x.sample(frac=fraction, random_state=random_seed))\n",
    "    \n",
    "    # Perform the appropriate subsampling based on the specified type\n",
    "    if subsample_type == 'class':\n",
    "        subset = sample_by_class(df, subset_fraction)\n",
    "    elif subsample_type == 'continuous':\n",
    "        subset = sample_by_continuous(df, subset_fraction)\n",
    "    elif subsample_type == 'random':\n",
    "        subset = df.sample(frac=subset_fraction, random_state=random_seed)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid subsample_type. Choose from 'class', 'continuous', or 'random'.\")\n",
    "\n",
    "    # Save the subset to the original CSV file\n",
    "    subset.to_csv(path, index=False)\n",
    "    \n",
    "    # Print statement to confirm subsampling\n",
    "    print(f\"File has been subsampled and saved: {path}\")\n",
    "\n",
    "# Example usage\n",
    "for data_set in [\"train\", \"val\", \"test\"]:\n",
    "    subsample_data(path=f\"data/FLIP/scl/{data_set}/x000.csv\", subsample_type='class', column='scl_label', random_seed=42)\n",
    "    subsample_data(path=f\"data/FLIP/meltome/{data_set}/x000.csv\", subsample_type='continuous', column='target', random_seed=42)\n",
    "    subsample_data(path=f\"data/FLIP/secondary_structure/{data_set}/x000.csv\", subsample_type='random', random_seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "\n",
    "The BioNeMo framework supports easy fine-tuning on downstream tasks by loading the pretrained model, which can be frozen or unfrozen, and adding a task-specific head. BioNeMo also provides example config files for downstream task fine-tuning of ESM-2nv and ESM-1nv on some FLIP tasks.\n",
    "\n",
    "A pretrained ESM model can be provided using a path to a NeMo model (via `restore_encoder_path`). This is done through:\n",
    "\n",
    "* Adding `model.restore_encoder_path:` To the config yaml.\n",
    "* Passing `model.restore_encoder_path:` As a command-line argument into your script.\n",
    "\n",
    "#### Method 1: Standard Fine-Tuning for Sequence Level Classification Tasks\n",
    "In this example, we will predict the 10 subcellular localization sites of proteins as described in the FLIP dataset. Under the `data/FLIP/scl` folder, you will see the correct expected structure for BioNeMo:\n",
    "\n",
    "````shell\n",
    "data/path/\n",
    "    train/\n",
    "        x000.csv\n",
    "    val/\n",
    "        x000.csv\n",
    "    test/\n",
    "        x000.csv\n",
    "````\n",
    "\n",
    "By inspecting the file, you will see three columns:\n",
    "- `id`: The sequence ID.\n",
    "- `sequence`: The protein sequence.\n",
    "- `target`: The corresponding class of the sequence.\n",
    "\n",
    "The CSV files should be named exactly as `x000.csv`. You can provide a list of such files by specifying it as a list in the config file. For instance, if you have 50 csv files, you can specify this by setting `x[000..049]` to take files named `x000.csv` up to a file named `x0049.csv`.\n",
    "\n",
    "To run this downstream task, we have included an example `downstream_flip_scl` configuration file. For your own custom downstream tasks, you can create your own YAML file or override existing ones using HYDRA by specifying the following fields:\n",
    "\n",
    "- `restore_from_path`: Set to the path of the pretrained model checkpoint `.nemo` file.\n",
    "- `trainer.devices`, `trainer.num_nodes`: Set it to the number of GPU and nodes, respectively.\n",
    "- `trainer.max_epochs`: Set to the number of epochs you want to train.\n",
    "- `trainer.val_check_interval`: Set to the number of steps to run validation.\n",
    "- `model.micro_batch_size`: Set to the micro batch size for training.\n",
    "- `data.task_name`: Can be anything.\n",
    "- `data.task_type`: The current options are `token-level-classification`, `classification` (sequence level), and `regression` (sequence level).\n",
    "- `preprocessed_data_path`: Set to the path of the parent folder of `dataset_path`. See - `dataset_path` for how this env is used.\n",
    "- `dataset_path`: Set to the folder that contains `train`/`val`/`test` folders.\n",
    "- `dataset.train`, `dataset.val`, `dataset.test`: Set to the CSV name or ranges.\n",
    "- `sequence_column`: Set to the name of the column containing the sequence, e.g. `sequence` in this example.\n",
    "- `target_column`: Set to the name of the column containing the target, e.g. `scl_label` in this example.\n",
    "- `target_size`: Number of classes in each label for classification.\n",
    "- `num_classes`: Set to `target_size`.\n",
    "- `encoder_frozen`: Used to set the encoder trainable or frozen, True by default.\n",
    "\n",
    "This task will use the `CrossEntropyLoss` and add an `MLPmodel` task head with `ReLU` as activation function, `LayerNorm`, and `Dropout` set at `0.25` as specified in the  `../model/core/mlp_model.py` file.\n",
    "\n",
    "For the purpose of this demo example, we will shorten the time required for training by setting the following parameters: `++trainer.max_steps=1`, `++val_check_interval=1`, `++limit_val_batches` and `++limit_test_batches`, reducing the number of batches for validation and testing to 1. Users can update these parameters by editing the `.yaml` config file or by overriding config arguments at runtime using Hydra, as shown in the example below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>scl_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sequence867</td>\n",
       "      <td>MQGSKGVENPAFVPSSPDTPRRASASPSQVEVSAVASRNQNGGSQP...</td>\n",
       "      <td>Cell_membrane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sequence439</td>\n",
       "      <td>MNVSHASVHPVEDPPAAATEVENPPRVRMDDMEGMPGTLLGLALRF...</td>\n",
       "      <td>Cell_membrane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sequence342</td>\n",
       "      <td>MKMASSLAFLLLNFHVSLFLVQLLTPCSAQFSVLGPSGPILAMVGE...</td>\n",
       "      <td>Cell_membrane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sequence735</td>\n",
       "      <td>MENPPNETEAKQIQTNEGKKTKGGIITMPFIIANEAFEKVASYGLL...</td>\n",
       "      <td>Cell_membrane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sequence784</td>\n",
       "      <td>MKSFNTEGHNHSTAESGDAYTVSDPTKNVDEDGREKRTGTWLTASA...</td>\n",
       "      <td>Cell_membrane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                           sequence  \\\n",
       "0  Sequence867  MQGSKGVENPAFVPSSPDTPRRASASPSQVEVSAVASRNQNGGSQP...   \n",
       "1  Sequence439  MNVSHASVHPVEDPPAAATEVENPPRVRMDDMEGMPGTLLGLALRF...   \n",
       "2  Sequence342  MKMASSLAFLLLNFHVSLFLVQLLTPCSAQFSVLGPSGPILAMVGE...   \n",
       "3  Sequence735  MENPPNETEAKQIQTNEGKKTKGGIITMPFIIANEAFEKVASYGLL...   \n",
       "4  Sequence784  MKSFNTEGHNHSTAESGDAYTVSDPTKNVDEDGREKRTGTWLTASA...   \n",
       "\n",
       "       scl_label  \n",
       "0  Cell_membrane  \n",
       "1  Cell_membrane  \n",
       "2  Cell_membrane  \n",
       "3  Cell_membrane  \n",
       "4  Cell_membrane  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration_folder = os.path.join(bionemo_home, f'examples/protein/{model_name}/conf')\n",
    "scl_df = pd.read_csv(f'{bionemo_home}/data/FLIP/scl/train/x000.csv')\n",
    "scl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "!cd {bionemo_home} && python examples/protein/downstream/downstream_flip.py \\\n",
    "    --config-path={configuration_folder} \\\n",
    "    --config-name=downstream_flip_scl \\\n",
    "    name={model_name}-finetuned-scl \\\n",
    "    ++trainer.devices=1 \\\n",
    "    ++trainer.max_epochs=1 \\\n",
    "    ++trainer.val_check_interval=1 \\\n",
    "    ++trainer.limit_test_batches=1 \\\n",
    "    ++trainer.limit_val_batches=1 \\\n",
    "    ++model.micro_batch_size=1 \\\n",
    "    ++trainer.max_steps=1 \\\n",
    "    ++exp_manager.create_wandb_logger=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Standard Fine-Tuning for Sequence-Level Regression Tasks\n",
    "\n",
    "In this example, we will predict the melting temperature of proteins as described in the FLIP dataset.\n",
    "\n",
    "As before, we need our files to be in the correct format with the appropriate naming. Thanks to the preprocessing steps we carried out at the beginning of this notebook, the data is already in the right format. Again, we have a custom `downstream_flip_meltome.yaml` configuration file with all the correct settings.\n",
    "\n",
    "Inside it, you should pay attention to:\n",
    "\n",
    "- **`loss_func`**: This time it is `MSELoss`.\n",
    "- **`task_name`**: `meltome`.\n",
    "- **`sequence_column`**: The name of the column where the protein sequence is located.\n",
    "- **`target_column`**: This is the target column in our file, which is called `target`.\n",
    "- **`target_sizes`**: This is the number of classes in each label; in this case, it will be 1, as it is a regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sequence10771</td>\n",
       "      <td>MSWPTLTVRLQQKVIRYLDYESRCNLRICSKDDKDSVDSVKFNPKT...</td>\n",
       "      <td>35.898600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sequence3319</td>\n",
       "      <td>MRLVKQEYVLDGLDCSNCARKIENGVKGIKGINGCAVNFAASTLTV...</td>\n",
       "      <td>38.732746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sequence3630</td>\n",
       "      <td>MSSFDRRIEAACKFDDERYYKQYHRYFDVLAQVHSVVETINGAQML...</td>\n",
       "      <td>39.144778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sequence8135</td>\n",
       "      <td>MDAEDGFDPTLLKKKKKKKTTFDLDAALGLEDDTKKEDPQDEASAE...</td>\n",
       "      <td>40.476706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sequence3437</td>\n",
       "      <td>MSYYNKRNQEPLPKEDVSTWECTKEDCNGWTRKNFASSDTPLCPLC...</td>\n",
       "      <td>36.754142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           sequence     target\n",
       "0  Sequence10771  MSWPTLTVRLQQKVIRYLDYESRCNLRICSKDDKDSVDSVKFNPKT...  35.898600\n",
       "1   Sequence3319  MRLVKQEYVLDGLDCSNCARKIENGVKGIKGINGCAVNFAASTLTV...  38.732746\n",
       "2   Sequence3630  MSSFDRRIEAACKFDDERYYKQYHRYFDVLAQVHSVVETINGAQML...  39.144778\n",
       "3   Sequence8135  MDAEDGFDPTLLKKKKKKKTTFDLDAALGLEDDTKKEDPQDEASAE...  40.476706\n",
       "4   Sequence3437  MSYYNKRNQEPLPKEDVSTWECTKEDCNGWTRKNFASSDTPLCPLC...  36.754142"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meltome_df = pd.read_csv(f'{bionemo_home}/data/FLIP/meltome/train/x000.csv')\n",
    "meltome_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sequence10771</td>\n",
       "      <td>MSWPTLTVRLQQKVIRYLDYESRCNLRICSKDDKDSVDSVKFNPKT...</td>\n",
       "      <td>35.898600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sequence3319</td>\n",
       "      <td>MRLVKQEYVLDGLDCSNCARKIENGVKGIKGINGCAVNFAASTLTV...</td>\n",
       "      <td>38.732746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sequence3630</td>\n",
       "      <td>MSSFDRRIEAACKFDDERYYKQYHRYFDVLAQVHSVVETINGAQML...</td>\n",
       "      <td>39.144778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sequence8135</td>\n",
       "      <td>MDAEDGFDPTLLKKKKKKKTTFDLDAALGLEDDTKKEDPQDEASAE...</td>\n",
       "      <td>40.476706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sequence3437</td>\n",
       "      <td>MSYYNKRNQEPLPKEDVSTWECTKEDCNGWTRKNFASSDTPLCPLC...</td>\n",
       "      <td>36.754142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           sequence     target\n",
       "0  Sequence10771  MSWPTLTVRLQQKVIRYLDYESRCNLRICSKDDKDSVDSVKFNPKT...  35.898600\n",
       "1   Sequence3319  MRLVKQEYVLDGLDCSNCARKIENGVKGIKGINGCAVNFAASTLTV...  38.732746\n",
       "2   Sequence3630  MSSFDRRIEAACKFDDERYYKQYHRYFDVLAQVHSVVETINGAQML...  39.144778\n",
       "3   Sequence8135  MDAEDGFDPTLLKKKKKKKTTFDLDAALGLEDDTKKEDPQDEASAE...  40.476706\n",
       "4   Sequence3437  MSYYNKRNQEPLPKEDVSTWECTKEDCNGWTRKNFASSDTPLCPLC...  36.754142"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meltome_df = pd.read_csv(f'{bionemo_home}/data/FLIP/meltome/train/x000.csv')\n",
    "meltome_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "!cd {bionemo_home} && python examples/protein/downstream/downstream_flip.py \\\n",
    "    --config-path={configuration_folder} \\\n",
    "    --config-name=downstream_flip_meltome \\\n",
    "    name={model_name}-finetuned-meltome \\\n",
    "    ++trainer.devices=1 \\\n",
    "    ++trainer.max_epochs=1 \\\n",
    "    ++trainer.val_check_interval=1 \\\n",
    "    ++model.micro_batch_size=1 \\\n",
    "    ++trainer.max_steps=1 \\\n",
    "    ++exp_manager.create_wandb_logger=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 3: Standard finetune for token level classification tasks\n",
    "\n",
    "In this example, we will predict three structure states of proteins, as described in the FLIP dataset. For each amino acid in the sequence, the model predicts whether it is found in a helix, sheet, or coil structure.\n",
    "\n",
    "- For the target column (e.g. `3state`), use a sequence of the same length as the protein sequence. Each character in the sequence represents a class (e.g. `C` for coil, `H` for helix, `E` for sheet)\n",
    "- You can also apply a mask column. For example, the `resolved` column uses a sequence of 1 and 0s that is the same length of the protein sequence. 1 = experimentally resolved, 0 = not resolved.\n",
    "- The loss will only be calculated for the resolved positions as this is specified under the `mask_column`.\n",
    "- The `loss_fn` no longer needs to be set as it is pre-built in the `/model/protein/downstream/protein_model_finetuning.py` under `build_loss_fn`. The `PerTokenMaskedCrossEntropyLoss` in this function is further defined in `/model/core/cnn.py`\n",
    "\n",
    "You can have multiple target columns in the same dataset by setting them as a list under the `target_column`, for instance for this task you can have:\n",
    "\n",
    "- `target_column`: [\"3state\", \"8state\"]\n",
    "- `target_size`: [3, 8]\n",
    "- `mask_column`: [\"resolved\", \"resolved\"]\n",
    "\n",
    "In doing so, the loss will be calculated for both columns.\n",
    "\n",
    "You can remove tha masking by setting `mask_column`: [null]\n",
    "\n",
    "In this instance, as we are doing a token level classification task, we will be attaching a `ConvNet` head based on `../bionemo/model/core/cnn.py` which uses the `PerTokenMaskedCrossEntropyLoss` class as loss function with `ReLU` as activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>3state</th>\n",
       "      <th>resolved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5kar-A</td>\n",
       "      <td>DRHHHHHHKLQLGRFWHISDLHLDPNYTVSKDPLQVCPSAGSQPVL...</td>\n",
       "      <td>CCCCCCCCCCCCEEEEEECCCCECCCCCCCCCCCCCCHHHCCCCCC...</td>\n",
       "      <td>0000000000011111111111111111111111111111111111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1j2j-B</td>\n",
       "      <td>NVIFEDEEKSKMLARLLKSSHPEDLRAANKLIKEMVQEDQKRMEK</td>\n",
       "      <td>CCCCCCHHHHHHHHHHHCCCCHHHHHHHHHHHHHHHHHHHCCCCC</td>\n",
       "      <td>001111111111111111111111111111111111111111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5x6s-B</td>\n",
       "      <td>SGSLQQVTDFGDNPTNVGMYIYVPNNLASNPGIVVAIHYCTGTGPG...</td>\n",
       "      <td>CCEEEEECCCCCCCCCCEEEEEECCCCCCCCCEEEEECCCCCCHHH...</td>\n",
       "      <td>0111111111111111111111111111111111111111111111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5jrc-C</td>\n",
       "      <td>MGSSHHHHHHSSGLVPRGSHMASMTGGQQMGRGSMLPNLDNLKEEY...</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCHHHHHHHH...</td>\n",
       "      <td>0000000000000000000000000000000001111111111111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2rjo-A</td>\n",
       "      <td>MSLGQTTLACSFRSLTNPYYTAFNKGAQSFAKSVGLPYVPLTTEGS...</td>\n",
       "      <td>CCCCCCEEEEEECCCCCHHHHHHHHHHHHHHHHHCCCEEEEECCCC...</td>\n",
       "      <td>0011111111111111111111111111111111111111111111...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                           sequence  \\\n",
       "0  5kar-A  DRHHHHHHKLQLGRFWHISDLHLDPNYTVSKDPLQVCPSAGSQPVL...   \n",
       "1  1j2j-B      NVIFEDEEKSKMLARLLKSSHPEDLRAANKLIKEMVQEDQKRMEK   \n",
       "2  5x6s-B  SGSLQQVTDFGDNPTNVGMYIYVPNNLASNPGIVVAIHYCTGTGPG...   \n",
       "3  5jrc-C  MGSSHHHHHHSSGLVPRGSHMASMTGGQQMGRGSMLPNLDNLKEEY...   \n",
       "4  2rjo-A  MSLGQTTLACSFRSLTNPYYTAFNKGAQSFAKSVGLPYVPLTTEGS...   \n",
       "\n",
       "                                              3state  \\\n",
       "0  CCCCCCCCCCCCEEEEEECCCCECCCCCCCCCCCCCCHHHCCCCCC...   \n",
       "1      CCCCCCHHHHHHHHHHHCCCCHHHHHHHHHHHHHHHHHHHCCCCC   \n",
       "2  CCEEEEECCCCCCCCCCEEEEEECCCCCCCCCEEEEECCCCCCHHH...   \n",
       "3  CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCHHHHHHHH...   \n",
       "4  CCCCCCEEEEEECCCCCHHHHHHHHHHHHHHHHHCCCEEEEECCCC...   \n",
       "\n",
       "                                            resolved  \n",
       "0  0000000000011111111111111111111111111111111111...  \n",
       "1      001111111111111111111111111111111111111111100  \n",
       "2  0111111111111111111111111111111111111111111111...  \n",
       "3  0000000000000000000000000000000001111111111111...  \n",
       "4  0011111111111111111111111111111111111111111111...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secondary_structure_df = pd.read_csv(f'{bionemo_home}/data/FLIP/secondary_structure/train/x000.csv')\n",
    "secondary_structure_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>3state</th>\n",
       "      <th>resolved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5kar-A</td>\n",
       "      <td>DRHHHHHHKLQLGRFWHISDLHLDPNYTVSKDPLQVCPSAGSQPVL...</td>\n",
       "      <td>CCCCCCCCCCCCEEEEEECCCCECCCCCCCCCCCCCCHHHCCCCCC...</td>\n",
       "      <td>0000000000011111111111111111111111111111111111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1j2j-B</td>\n",
       "      <td>NVIFEDEEKSKMLARLLKSSHPEDLRAANKLIKEMVQEDQKRMEK</td>\n",
       "      <td>CCCCCCHHHHHHHHHHHCCCCHHHHHHHHHHHHHHHHHHHCCCCC</td>\n",
       "      <td>001111111111111111111111111111111111111111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5x6s-B</td>\n",
       "      <td>SGSLQQVTDFGDNPTNVGMYIYVPNNLASNPGIVVAIHYCTGTGPG...</td>\n",
       "      <td>CCEEEEECCCCCCCCCCEEEEEECCCCCCCCCEEEEECCCCCCHHH...</td>\n",
       "      <td>0111111111111111111111111111111111111111111111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5jrc-C</td>\n",
       "      <td>MGSSHHHHHHSSGLVPRGSHMASMTGGQQMGRGSMLPNLDNLKEEY...</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCHHHHHHHH...</td>\n",
       "      <td>0000000000000000000000000000000001111111111111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2rjo-A</td>\n",
       "      <td>MSLGQTTLACSFRSLTNPYYTAFNKGAQSFAKSVGLPYVPLTTEGS...</td>\n",
       "      <td>CCCCCCEEEEEECCCCCHHHHHHHHHHHHHHHHHCCCEEEEECCCC...</td>\n",
       "      <td>0011111111111111111111111111111111111111111111...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                           sequence  \\\n",
       "0  5kar-A  DRHHHHHHKLQLGRFWHISDLHLDPNYTVSKDPLQVCPSAGSQPVL...   \n",
       "1  1j2j-B      NVIFEDEEKSKMLARLLKSSHPEDLRAANKLIKEMVQEDQKRMEK   \n",
       "2  5x6s-B  SGSLQQVTDFGDNPTNVGMYIYVPNNLASNPGIVVAIHYCTGTGPG...   \n",
       "3  5jrc-C  MGSSHHHHHHSSGLVPRGSHMASMTGGQQMGRGSMLPNLDNLKEEY...   \n",
       "4  2rjo-A  MSLGQTTLACSFRSLTNPYYTAFNKGAQSFAKSVGLPYVPLTTEGS...   \n",
       "\n",
       "                                              3state  \\\n",
       "0  CCCCCCCCCCCCEEEEEECCCCECCCCCCCCCCCCCCHHHCCCCCC...   \n",
       "1      CCCCCCHHHHHHHHHHHCCCCHHHHHHHHHHHHHHHHHHHCCCCC   \n",
       "2  CCEEEEECCCCCCCCCCEEEEEECCCCCCCCCEEEEECCCCCCHHH...   \n",
       "3  CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCHHHHHHHH...   \n",
       "4  CCCCCCEEEEEECCCCCHHHHHHHHHHHHHHHHHCCCEEEEECCCC...   \n",
       "\n",
       "                                            resolved  \n",
       "0  0000000000011111111111111111111111111111111111...  \n",
       "1      001111111111111111111111111111111111111111100  \n",
       "2  0111111111111111111111111111111111111111111111...  \n",
       "3  0000000000000000000000000000000001111111111111...  \n",
       "4  0011111111111111111111111111111111111111111111...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secondary_structure_df = pd.read_csv(f'{bionemo_home}/data/FLIP/secondary_structure/train/x000.csv')\n",
    "secondary_structure_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "!cd {bionemo_home} && python examples/protein/downstream/downstream_flip.py \\\n",
    "    --config-path={configuration_folder} \\\n",
    "    --config-name=downstream_flip_sec_str \\\n",
    "    name={model_name}-finetuned-sec-str \\\n",
    "    ++trainer.devices=1 \\\n",
    "    ++trainer.max_epochs=1 \\\n",
    "    ++trainer.val_check_interval=1 \\\n",
    "    ++model.micro_batch_size=1 \\\n",
    "    ++trainer.max_steps=1 \\\n",
    "    ++exp_manager.create_wandb_logger=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 4: LoRA Fine-Tuning for Token-Level Classification Task\n",
    "\n",
    "In this example, we will replicate the fine-tuning using the 3-state structure of proteins with LoRA adapters.\n",
    "\n",
    "**Low-Rank Adaptation (LoRA)** is a parameter-efficient strategy designed to adapt large pretrained language models to downstream tasks while avoiding challenges associated with full fine-tuning. Unlike traditional fine-tuning approaches that adjust all parameters within a pretrained model, LoRA maintains the core weights of the pretrained model as frozen. Instead, it introduces trainable rank decomposition matrices, known as LoRA adapters, into each layer of the Transformer architecture. These adapters are smaller matrices that approximate the original weight matrices, thereby reducing the number of trainable parameters.\n",
    "\n",
    "In the context of antibody sequences, where data availability may be limited, LoRA offers several advantages. By focusing on adapting these smaller adapter matrices rather than the entire model, LoRA makes fine-tuning more efficient and less susceptible to overfitting. This is particularly beneficial for tasks requiring adaptation to specific protein sequences, where preserving the learned features of the pretrained ESM-2nv model is crucial.\n",
    "\n",
    "By integrating LoRA into BioNeMo's fine-tuning pipeline for ESM-2nv models, you can leverage the robustness of pretrained models while tailoring them to the unique characteristics of antibody sequences. This extension not only enhances model performance but also ensures adaptability and efficiency in handling specialized protein sequence data.\n",
    "\n",
    "***Key Adjustments in the YAML File***:\n",
    "\n",
    "- **`model.peft.enabled=True`**: Enables the PEFT (Parameter-Efficient Fine-Tuning) technique, specifically using LoRA (`lora`).\n",
    "  \n",
    "- **`model.peft.peft_scheme=\"lora\"`**: Specifies that LoRA is used as the adaptation method.\n",
    "  \n",
    "- **`++model.peft.lora_tuning.adapter_dim=32`**: Sets the dimensionality of the adapter layers used in LoRA.\n",
    "\n",
    "- **`++model.peft.lora_tuning.adapter_dropout=0.0`**: Specifies the dropout rate for the adapter layers in LoRA.\n",
    "  \n",
    "- **`++model.peft.lora_tuning.column_init_method=\"xavier\"`**: Defines the initialization method for the column weights of the adapter layers in LoRA.\n",
    "\n",
    "- **`++model.peft.lora_tuning.row_init_method=\"zero\"`**: Specifies the initialization method for the row weights of the adapter layers in LoRA.\n",
    "\n",
    "- **`++model.peft.lora_tuning.layer_selection=null`**: Determines which layers to apply LoRA adapters to. If `null`, adapters are applied to all layers.\n",
    "\n",
    "- **`++model.peft.lora_tuning.weight_tying=False`**: Specifies whether weight tying is used in LoRA.\n",
    "\n",
    "- **`++model.peft.lora_tuning.position_embedding_strategy=null`**: Used only when `weight_tying` is `True`. Specifies the strategy for position embeddings in LoRA.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> LoRA is currently not supported for ESM-1nv.</div>\n",
    "\n",
    "Following [these instructions](/bionemo/docs/bionemo/lora-finetuning-esm2.md) and reimplementing the `ESM2nvLoRAModel` class in the `bionemo/model/protein/esm1nv/esm1nv_model.py` script for ESM-1, you can perform LoRA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "! cd /workspace/bionemo && python examples/protein/downstream/downstream_flip.py \\\n",
    "    --config-path={configuration_folder} \\\n",
    "    --config-name=downstream_sec_str_LORA \\\n",
    "     name={model_name}-finetuned-sec-str_LORA \\\n",
    "    ++trainer.devices=1 \\\n",
    "    ++trainer.max_epochs=1 \\\n",
    "    ++trainer.val_check_interval=1 \\\n",
    "    ++model.micro_batch_size=1 \\\n",
    "    ++trainer.max_steps=1 \\\n",
    "    ++exp_manager.create_wandb_logger=false \\\n",
    "    ++exp_manager.resume_if_exists=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we have learned how to use the existing preprocessing script for the FLIP dataset, perform fine-tuning for different downstream tasks, and apply LoRA adaptors on the FLIP data. In [this other notebook](./esm2_paratope_finetuning.ipynb), you will see how to bring your own data for fine-tuning purposes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
