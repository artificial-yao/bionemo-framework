{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demostrates the creation of a model in bionemo. \n",
    "Models need to be megatron modules. At the most basic level this just means:\n",
    "  1. They need a config argument of type megatron.core.ModelParallelConfig. An easy way of implementing this is to inherit from bionemo.llm.model.config.MegatronBioNeMoTrainableModelConfig. This is a class for bionemo that supports usage with Megatron models, as NeMo2 requires. This class also inherits ModelParallelConfig. \n",
    "  2. They need a self.model_type:megatron.core.transformer.enums.ModelType enum defined (ModelType.encoder_or_decoder is probably usually fine)\n",
    "  3. def set_input_tensor(self, input_tensor) needs to be present. This is used in model parallelism. This function can be a stub/ placeholder function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from nemo.lightning.megatron_parallel import MegatronLossReduction\n",
    "from torchvision.datasets import MNIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Losses: here we define a simple loss function. These should inherit from losses in nemo.lightning.megatron_parallel. \n",
    "The output of forward and backwared passes happen in parallel.\n",
    "The reduce function is required. It is only used for collecting forward output for inference, as well as for logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MSELossReduction(MegatronLossReduction):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets used for model training must be compatible with megatron datasets.\n",
    "The dataset modules must have a data_sampler in it which is a nemo2 peculiarity. Also the sampler will not shuffle your data! So you need to wrap your dataset in a dataset shuffler that maps sequential ids to random ids in your dataset. For further information, see: docs/user-guide/background/megatron_datasets.md. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MnistItem(TypedDict):\n",
    "    data: Tensor\n",
    "    label: Tensor\n",
    "    idx: int\n",
    "    \n",
    "class MNISTCustom(MNIST):\n",
    "    def __getitem__(self, index: int) -> MnistItem:\n",
    "        \"\"\"Wraps the getitem method of the MNIST dataset such that we return a Dict\n",
    "        instead of a Tuple or tensor.\n",
    "\n",
    "        Args:\n",
    "            index: The index we want to grab, an int.\n",
    "\n",
    "        Returns:\n",
    "            A dict containing the data (\"x\"), label (\"y\"), and index (\"idx\").\n",
    "        \"\"\"  # noqa: D205\n",
    "        x, y = super().__getitem__(index)\n",
    "\n",
    "        return {\n",
    "            \"data\": x,\n",
    "            \"label\": y,\n",
    "            \"idx\": index,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
