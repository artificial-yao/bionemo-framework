{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demostrates the creation of a model in bionemo. \n",
    "`Megatron` / `NeMo` modules and datasets are special derivatives of PyTorch modules and datasets that extend and accelerate the distributed training and inference capabilities of PyTorch.\n",
    "\n",
    "Some distinctions of Megatron / NeMo are:\n",
    "\n",
    "- `torch.nn.Module`/`LightningModule` changes into `MegatronModule`.\n",
    "- Loss functions should extend the `MegatronLossReduction` module and implement a `reduce` method for aggregating loss across multiple micro-batches.\n",
    "- Megatron configuration classes (e.g. `megatron.core.transformer.TransformerConfig`) are extended with a `configure_model` method that defines how model weights are initialized and loaded in a way that is compliant with training via NeMo2.\n",
    "- Various modifications and extensions to common PyTorch classes, such as adding a `MegatronDataSampler` (and re-sampler such as `PRNGResampleDataset` or `MultiEpochDatasetResampler`) to your `LightningDataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from nemo.lightning.megatron_parallel import MegatronLossReduction\n",
    "from torchvision.datasets import MNIST\n",
    "from nemo.lightning.pytorch.plugins import MegatronDataSampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Losses: here we define a simple loss function. These should inherit from losses in nemo.lightning.megatron_parallel. \n",
    "The output of forward and backwared passes happen in parallel.\n",
    "The reduce function is required. It is only used for collecting forward output for inference, as well as for logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MSELossReduction(MegatronLossReduction):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MnistItem(TypedDict):\n",
    "    data: Tensor\n",
    "    label: Tensor\n",
    "    idx: int\n",
    "    \n",
    "class MNISTCustom(MNIST):\n",
    "    def __getitem__(self, index: int) -> MnistItem:\n",
    "        \"\"\"Wraps the getitem method of the MNIST dataset such that we return a Dict\n",
    "        instead of a Tuple or tensor.\n",
    "\n",
    "        Args:\n",
    "            index: The index we want to grab, an int.\n",
    "\n",
    "        Returns:\n",
    "            A dict containing the data (\"x\"), label (\"y\"), and index (\"idx\").\n",
    "        \"\"\"  # noqa: D205\n",
    "        x, y = super().__getitem__(index)\n",
    "\n",
    "        return {\n",
    "            \"data\": x,\n",
    "            \"label\": y,\n",
    "            \"idx\": index,\n",
    "        }\n",
    "mnist_full = MNISTCustom(self.data_dir, download=True, transform=transforms.ToTensor(), train=True)\n",
    "mnist_train_data, mnist_val_data = torch.utils.data.random_split(\n",
    "            mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "mnist_test_set_data = \n",
    "            MNISTCustom(self.data_dir, download=True, transform=transforms.ToTensor(), train=False), seed=43)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets used for model training must be compatible with megatron datasets.\n",
    "The dataset modules must have a data_sampler in it which is a nemo2 peculiarity. Also the sampler will not shuffle your data! So you need to wrap your dataset in a dataset shuffler that maps sequential ids to random ids in your dataset. This is what PRNGResampleDataset does. For further information, see: docs/user-guide/background/megatron_datasets.md. Moreover, the compatability of datasets with megatron can be checked by running bionemo.testing.data_utils.assert_dataset_compatible_with_megatron.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "mnist_test = PRNGResampleDataset(mnist_test_set_data)\n",
    "mnist_train = PRNGResampleDataset(mnist_train_set_data)\n",
    "mnist_val = PRNGResampleDataset(mnist_val_set_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data module can now take these in as inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "In the data module class, it's necessary to have data_sampler set. This data sampler that can be used to shuffle the data. A MegatronDataSampler is the best choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MNISTDataModule(pl.LightningDataModule): \n",
    "    def __init__(self, data_sampler:MegatronDataSampler, mnist_train:PRNGResampleDataset, mnist_val:PRNGResampleDataset, mnist_test:PRNGResampleDataset, batch_size: int = 32) -> None:  # noqa: D107\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.mnist_train = mnist_train\n",
    "        self.mnist_test = mnist_test\n",
    "        self.mnist_val = mnist_val\n",
    "        #  Note that this sampler is sequential, meaning it does not do any shuffling. Let's wrap our data in a shuffler.\n",
    "        # Wraps the datasampler with the MegatronDataSampler. The MegatronDataSampler is a wrapper that allows the sampler\n",
    "        # to be used with megatron. It sets up the capability to utilize micro-batching and gradient accumulation. It is also\n",
    "        # the place where the global batch size is constructed.\n",
    "        self.data_sampler = MegatronDataSampler(\n",
    "            seq_len=self.max_len,\n",
    "            micro_batch_size=self.batch_size,\n",
    "            global_batch_size=self.batch_size,\n",
    "            rampup_batch_size=None,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:  # noqa: D102\n",
    "        return DataLoader(self.mnist_train, batch_size=self.micro_batch_size, num_workers=0)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:  # noqa: D102\n",
    "        return DataLoader(self.mnist_val, batch_size=self.micro_batch_size, num_workers=0)\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:  # noqa: D102\n",
    "        return DataLoader(self.mnist_test, batch_size=self.micro_batch_size, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models need to be megatron modules. At the most basic level this just means:\n",
    "  1. They need a config argument of type megatron.core.ModelParallelConfig. An easy way of implementing this is to inherit from bionemo.llm.model.config.MegatronBioNeMoTrainableModelConfig. This is a class for bionemo that supports usage with Megatron models, as NeMo2 requires. This class also inherits ModelParallelConfig. \n",
    "  2. They need a self.model_type:megatron.core.transformer.enums.ModelType enum defined (ModelType.encoder_or_decoder is probably usually fine)\n",
    "  3. def set_input_tensor(self, input_tensor) needs to be present. This is used in model parallelism. This function can be a stub/ placeholder function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
