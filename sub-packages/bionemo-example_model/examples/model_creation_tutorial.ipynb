{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demostrates the creation of a model in bionemo. \n",
    "`Megatron` / `NeMo` modules and datasets are special derivatives of PyTorch modules and datasets that extend and accelerate the distributed training and inference capabilities of PyTorch.\n",
    "\n",
    "Some distinctions of Megatron / NeMo are:\n",
    "\n",
    "- `torch.nn.Module`/`LightningModule` changes into `MegatronModule`.\n",
    "- Loss functions should extend the `MegatronLossReduction` module and implement a `reduce` method for aggregating loss across multiple micro-batches.\n",
    "- Megatron configuration classes (e.g. `megatron.core.transformer.TransformerConfig`) are extended with a `configure_model` method that defines how model weights are initialized and loaded in a way that is compliant with training via NeMo2.\n",
    "- Various modifications and extensions to common PyTorch classes, such as adding a `MegatronDataSampler` (and re-sampler such as `PRNGResampleDataset` or `MultiEpochDatasetResampler`) to your `LightningDataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from nemo.lightning.megatron_parallel import MegatronLossReduction\n",
    "from torchvision.datasets import MNIST\n",
    "from nemo.lightning.pytorch.plugins import MegatronDataSampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Losses: here we define a simple loss function. These should inherit from losses in nemo.lightning.megatron_parallel. \n",
    "The output of forward and backwared passes happen in parallel. The reduce function is required. It is only used for collecting forward output for inference, as well as for logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MSELossReduction(MegatronLossReduction):\n",
    "    \"\"\"A class used for calculating the loss, and for logging the reduced loss across micro batches.\"\"\"\n",
    "\n",
    "    def forward(self, batch: \"MnistItem\", forward_out: Dict[str, Tensor]) -> Tuple[Tensor, ReductionT]:\n",
    "        \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n",
    "\n",
    "        Args:\n",
    "            batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n",
    "            forward_out: the output of the forward method inside LitAutoEncoder.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing [<loss_tensor>, ReductionT] where the loss tensor will be used for\n",
    "                backpropagation and the ReductionT will be passed to the reduce method\n",
    "                (which currently only works for logging.).\n",
    "        \"\"\"\n",
    "        x = batch[\"data\"]\n",
    "        x_hat = forward_out[\"x_hat\"]\n",
    "        xview = x.view(x.size(0), -1).to(x_hat.dtype)\n",
    "        loss = nn.functional.mse_loss(x_hat, xview)\n",
    "\n",
    "        return loss, {\"avg\": loss}\n",
    "\n",
    "    def reduce(self, losses_reduced_per_micro_batch: Sequence[ReductionT]) -> Tensor:\n",
    "        \"\"\"Works across micro-batches. (data on single gpu).\n",
    "\n",
    "        Note: This currently only works for logging and this loss will not be used for backpropagation.\n",
    "\n",
    "        Args:\n",
    "            losses_reduced_per_micro_batch: a list of the outputs of forward\n",
    "\n",
    "        Returns:\n",
    "            A tensor that is the mean of the losses. (used for logging).\n",
    "        \"\"\"\n",
    "        mse_losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n",
    "        return mse_losses.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MnistItem(TypedDict):\n",
    "    data: Tensor\n",
    "    label: Tensor\n",
    "    idx: int\n",
    "    \n",
    "class MNISTCustom(MNIST):\n",
    "    def __getitem__(self, index: int) -> MnistItem:\n",
    "        \"\"\"Wraps the getitem method of the MNIST dataset such that we return a Dict\n",
    "        instead of a Tuple or tensor.\n",
    "\n",
    "        Args:\n",
    "            index: The index we want to grab, an int.\n",
    "\n",
    "        Returns:\n",
    "            A dict containing the data (\"x\"), label (\"y\"), and index (\"idx\").\n",
    "        \"\"\"  # noqa: D205\n",
    "        x, y = super().__getitem__(index)\n",
    "\n",
    "        return {\n",
    "            \"data\": x,\n",
    "            \"label\": y,\n",
    "            \"idx\": index,\n",
    "        }\n",
    "mnist_full = MNISTCustom(self.data_dir, download=True, transform=transforms.ToTensor(), train=True)\n",
    "mnist_train_data, mnist_val_data = torch.utils.data.random_split(\n",
    "            mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "mnist_test_set_data = \n",
    "            MNISTCustom(self.data_dir, download=True, transform=transforms.ToTensor(), train=False), seed=43)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets used for model training must be compatible with megatron datasets.\n",
    "The dataset modules must have a data_sampler in it which is a nemo2 peculiarity. Also the sampler will not shuffle your data! So you need to wrap your dataset in a dataset shuffler that maps sequential ids to random ids in your dataset. This is what PRNGResampleDataset does. For further information, see: docs/user-guide/background/megatron_datasets.md. Moreover, the compatability of datasets with megatron can be checked by running bionemo.testing.data_utils.assert_dataset_compatible_with_megatron.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "mnist_test = PRNGResampleDataset(mnist_test_set_data)\n",
    "mnist_train = PRNGResampleDataset(mnist_train_set_data)\n",
    "mnist_val = PRNGResampleDataset(mnist_val_set_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data module can now take these in as inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data module class, it's necessary to have data_sampler method to shuffle the data and that allows the sampler to be used with megatron. A nemo.lightning.pytorch.plugins.MegatronDataSampler is the best choice. It sets up the capability to utilize micro-batching and gradient accumulation. It is also the place where the global batch size is constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MNISTDataModule(pl.LightningDataModule): \n",
    "    def __init__(self, data_sampler:MegatronDataSampler, mnist_train:PRNGResampleDataset, mnist_val:PRNGResampleDataset, mnist_test:PRNGResampleDataset, batch_size: int = 32) -> None:  # noqa: D107\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.mnist_train = mnist_train\n",
    "        self.mnist_test = mnist_test\n",
    "        self.mnist_val = mnist_val\n",
    "        # Wraps the datasampler with the MegatronDataSampler. \n",
    "        self.data_sampler = MegatronDataSampler(\n",
    "            seq_len=self.max_len,\n",
    "            micro_batch_size=self.batch_size,\n",
    "            global_batch_size=self.batch_size,\n",
    "            rampup_batch_size=None,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:  # noqa: D102\n",
    "        return DataLoader(self.mnist_train, batch_size=self.micro_batch_size, num_workers=0)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:  # noqa: D102\n",
    "        return DataLoader(self.mnist_val, batch_size=self.micro_batch_size, num_workers=0)\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:  # noqa: D102\n",
    "        return DataLoader(self.mnist_test, batch_size=self.micro_batch_size, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model config class is used to instatiate the model. These configs must have:\n",
    "1. A configure_model function which allows the megatron strategy to lazily initialize the model after the parallel computing environment has been setup. These also handle loading starting weights for fine-tuning cases. Additionally these configs tell the trainer which loss you want to use with a matched model.\n",
    "2. A get_loss_reduction_class function that defines the loss fucntion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# typevar for capturing subclasses of ExampleModelTrunk. Useful for Generic type hints as below.\n",
    "ExampleModelT = TypeVar(\"ExampleModelT\", bound=ExampleModelTrunk)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExampleGenericConfig(Generic[ExampleModelT, Loss], MegatronBioNeMoTrainableModelConfig[ExampleModelT, Loss]):\n",
    "    \"\"\"ExampleConfig is a dataclass that is used to configure the model.\n",
    "\n",
    "    Timers from ModelParallelConfig are required for megatron forward compatibility.\n",
    "    \"\"\"\n",
    "\n",
    "    loss_cls: Type[Loss] = MSELossReduction \n",
    "    hidden_size: int = 64  # Needs to be set to avoid zero division error in megatron :(\n",
    "    num_attention_heads: int = 1  # Needs to be set to avoid zero division error in megatron :(\n",
    "    num_layers: int = 1  # Needs to be set to avoid zero division error in megatron :(\n",
    "    # IMPORTANT: Since we're adding/overriding the loss_cls, and that's not how we generally track this, we need to\n",
    "    #   add this into the list of config settings that we do not draw from the loaded checkpoint when restoring.\n",
    "    override_parent_fields: List[str] = field(default_factory=lambda: OVERRIDE_BIONEMO_CONFIG_DEFAULTS + [\"loss_cls\"])\n",
    "\n",
    "    def configure_model(self) -> ExampleModelT:\n",
    "        \"\"\"Uses model_cls and loss_cls to configure the model.\n",
    "\n",
    "        Note: Must pass self into Model since model requires having a config object.\n",
    "\n",
    "        Returns:\n",
    "            The model object.\n",
    "        \"\"\"\n",
    "        # 1. first load any settings that may exist in the checkpoint related to the model.\n",
    "        if self.initial_ckpt_path:\n",
    "            self.load_settings_from_checkpoint(self.initial_ckpt_path)\n",
    "        # 2. then initialize the model\n",
    "        model = self.model_cls(self)\n",
    "        # 3. Load weights from the checkpoint into the model\n",
    "        if self.initial_ckpt_path:\n",
    "            self.update_model_from_checkpoint(model, self.initial_ckpt_path)\n",
    "        return model\n",
    "\n",
    "    def get_loss_reduction_class(self) -> Type[Loss]:\n",
    "        \"\"\"Use loss_cls to configure the loss, since we do not change the settings of the loss based on the config.\"\"\"\n",
    "        return self.loss_cls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models need to be megatron modules. At the most basic level this just means:\n",
    "  1. They need a config argument of type megatron.core.ModelParallelConfig. An easy way of implementing this is to inherit from bionemo.llm.model.config.MegatronBioNeMoTrainableModelConfig. This is a class for bionemo that supports usage with Megatron models, as NeMo2 requires. This class also inherits ModelParallelConfig. \n",
    "  2. They need a self.model_type:megatron.core.transformer.enums.ModelType enum defined (ModelType.encoder_or_decoder is probably usually fine)\n",
    "  3. def set_input_tensor(self, input_tensor) needs to be present. This is used in model parallelism. This function can be a stub/ placeholder function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class ExampleModelTrunk(MegatronModule):\n",
    "    def __init__(self, config: ModelParallelConfig) -> None:\n",
    "        \"\"\"Constructor of the model.\n",
    "\n",
    "        Args:\n",
    "            config: The config object is responsible for telling the strategy what model to create.\n",
    "        \"\"\"\n",
    "        super().__init__(config)\n",
    "        # FIXME add an assertion that the user is not trying to do tensor parallelism since this doesn't use\n",
    "        #  parallelizable megatron linear layers.\n",
    "        self.model_type: ModelType = ModelType.encoder_or_decoder\n",
    "        self.linear1 = nn.Linear(28 * 28, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(64, 3)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tensor:\n",
    "        # we could return a dictionary of strings to tensors here, but let's demonstrate this is not necessary\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.linear1(x)\n",
    "        z = self.relu(z)\n",
    "        z = self.linear2(z)\n",
    "        return z\n",
    "\n",
    "    def set_input_tensor(self, input_tensor: Optional[Tensor]) -> None:\n",
    "        \"\"\"This _would_ be needed for model parallel and other kinds of more complicated forward passes in megatron.\"\"\"\n",
    "        pass\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
