{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "from scipy.stats import iqr\n",
    "from bionemo.scdl.io.single_cell_memmap_dataset import SingleCellMemMapDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import wraps\n",
    "import math\n",
    "from enum import Enum\n",
    "\n",
    "import subprocess\n",
    "from bionemo.scdl.util.torch_dataloader_utils import collate_sparse_matrix_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SingleCellMemMapDataset in module bionemo.scdl.io.single_cell_memmap_dataset:\n",
      "\n",
      "class SingleCellMemMapDataset(bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDataset)\n",
      " |  SingleCellMemMapDataset(data_path: str, h5ad_path: Optional[str] = None, num_elements: Optional[int] = None, num_rows: Optional[int] = None, mode: bionemo.scdl.io.single_cell_memmap_dataset.Mode = 'r+', dtypes: Dict[bionemo.scdl.io.single_cell_memmap_dataset.FileNames, str] = {'data.npy': 'float32', 'col_ptr.npy': 'uint32', 'row_ptr.npy': 'uint64'}) -> None\n",
      " |  \n",
      " |  Represents one or more AnnData matrices.\n",
      " |  \n",
      " |  Data is stored in large, memory-mapped arrays that enables fast access of\n",
      " |  datasets larger than the available amount of RAM on a system. SCMMAP\n",
      " |  implements a consistent API defined in SingleCellRowDataset.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      data_path: Location of np.memmap files to be loaded from or that will be\n",
      " |      created.\n",
      " |      mode: Whether the dataset will be read in (r+) from np.memmap files or\n",
      " |      written to np.memmap files (w+).\n",
      " |      data: A numpy array of the data\n",
      " |      row_index: A numpy array of row pointers\n",
      " |      col_index: A numpy array of column values\n",
      " |      metadata: Various metata about the dataset.\n",
      " |      _feature_index: The corresponding RowFeatureIndex where features are\n",
      " |      stored\n",
      " |      dtypes: A dictionary containing the datatypes of the data, row_index,\n",
      " |      and col_index arrays.\n",
      " |      _version: The version of the dataset\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SingleCellMemMapDataset\n",
      " |      bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDataset\n",
      " |      bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore\n",
      " |      abc.ABC\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, idx: int) -> torch.Tensor\n",
      " |      Get the row values located and index idx.\n",
      " |  \n",
      " |  __init__(self, data_path: str, h5ad_path: Optional[str] = None, num_elements: Optional[int] = None, num_rows: Optional[int] = None, mode: bionemo.scdl.io.single_cell_memmap_dataset.Mode = 'r+', dtypes: Dict[bionemo.scdl.io.single_cell_memmap_dataset.FileNames, str] = {'data.npy': 'float32', 'col_ptr.npy': 'uint32', 'row_ptr.npy': 'uint64'}) -> None\n",
      " |      Instantiate the class.\n",
      " |      \n",
      " |      Args:\n",
      " |          data_path: The location where the data np.memmap files are read from\n",
      " |          or stored.\n",
      " |          h5ad_path: Optional, the location of the h5_ad path.\n",
      " |          num_elements: The total number of elements in the array.\n",
      " |          num_rows: The number of rows in the data frame\n",
      " |          mode: Whether to read or write from the data_path,\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of rows.\n",
      " |  \n",
      " |  concat(self, other_dataset: Union[list['SingleCellMemMapDataset'], ForwardRef('SingleCellMemMapDataset')]) -> None\n",
      " |      Concatenates another SingleCellMemMapDataset to the existing one.\n",
      " |      \n",
      " |      The data is stored in the same place as for the original data set. This\n",
      " |      necessitates using _swap_memmap_array.\n",
      " |      \n",
      " |      Args:\n",
      " |          other_dataset: A SingleCellMemMapDataset or a list of\n",
      " |          SingleCellMemMapDatasets\n",
      " |      \n",
      " |      Raises:\n",
      " |         ValueError if the other dataset(s) are not of the same version or\n",
      " |         something of another type is passed in.\n",
      " |  \n",
      " |  features(self) -> Optional[bionemo.scdl.index.row_feature_index.RowFeatureIndex]\n",
      " |      Return the corresponding RowFeatureIndex.\n",
      " |  \n",
      " |  get_row(self, index: int, return_features: bool = False, feature_vars: Optional[List[str]] = None) -> Tuple[Tuple[numpy.ndarray, numpy.ndarray], pandas.core.frame.DataFrame]\n",
      " |      Returns a given row in the dataset along with optional features.\n",
      " |      \n",
      " |      Args:\n",
      " |          index: The row to be returned. This is in the range of [0, num_rows)\n",
      " |          return_features: boolean that indicates whether to return features\n",
      " |          feature_vars: Optional, feature variables to extract\n",
      " |      Return:\n",
      " |          [Tuple[np.ndarray, np.ndarray]: data values and column pointes\n",
      " |          pd.DataFrame: optional, corresponding features.\n",
      " |  \n",
      " |  get_row_column(self, index: int, column: int, impute_missing_zeros: bool = True) -> Optional[float]\n",
      " |      Returns the value at a given index and the corresponding column.\n",
      " |      \n",
      " |      Args:\n",
      " |          index: The index to be returned\n",
      " |          column: The column to be returned\n",
      " |          impute_missing_zeros: boolean that indicates whether to set missing\n",
      " |          data to 0\n",
      " |      Return:\n",
      " |          A float that is the value in the array or None.\n",
      " |  \n",
      " |  get_row_padded(self, index: int, return_features: bool = False, feature_vars: Optional[List[str]] = None) -> Tuple[numpy.ndarray, pandas.core.frame.DataFrame]\n",
      " |      Returns a padded version of a row in the dataset.\n",
      " |      \n",
      " |      A padded version is one where the a sparse array representation is\n",
      " |      converted to a conventional represenentation. Optionally, features are\n",
      " |      returned.\n",
      " |      \n",
      " |      Args:\n",
      " |          index: The row to be returned\n",
      " |          return_features: boolean that indicates whether to return features\n",
      " |          feature_vars: Optional, feature variables to extract\n",
      " |      Return:\n",
      " |          np.ndarray: conventional row representation\n",
      " |          pd.DataFrame: optional, corresponding features.\n",
      " |  \n",
      " |  load(self, stored_path: str) -> None\n",
      " |      Loads the data at store_path that is an np.memmap format.\n",
      " |      \n",
      " |      Args:\n",
      " |          stored_path: directory with np.memmap files\n",
      " |      Raises:\n",
      " |          FileNotFoundError if the corresponding directory or files are not\n",
      " |          found, or if the metadata file is not present.\n",
      " |  \n",
      " |  load_h5ad(self, anndata_path: str) -> None\n",
      " |      Loads an existing AnnData archive from disk.\n",
      " |      \n",
      " |      This creates a new backing data structure which is saved.\n",
      " |      Note: the storage utilized will roughly double. Currently, the data must\n",
      " |      be in a scipy.sparse.spmatrix format.\n",
      " |      \n",
      " |      Args:\n",
      " |          anndata_path: location of data load\n",
      " |      Raises:\n",
      " |          FileNotFoundError if the data path does not exist.\n",
      " |          NotImplementedError if the data is not in scipy.sparse.spmatrix\n",
      " |          format\n",
      " |          ValueError it there is not count data\n",
      " |  \n",
      " |  number_nonzero_values(self) -> int\n",
      " |      Number of non zero entries in the dataset.\n",
      " |  \n",
      " |  number_of_rows(self) -> int\n",
      " |      The number of rows in the dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The number of rows in the dataset\n",
      " |      Raises:\n",
      " |          ValueError if the length of the number of rows in the feature\n",
      " |          index does not correspond to the number of stored rows.\n",
      " |  \n",
      " |  number_of_values(self) -> int\n",
      " |      Get the total number of values in the array.\n",
      " |      \n",
      " |      For each index, the length of the corresponding dataframe is counted.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The sum of lengths of the features in every row\n",
      " |  \n",
      " |  number_of_variables(self) -> List[int]\n",
      " |      Get the number of features in every entry in the dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list containing the lengths of the features in every row\n",
      " |  \n",
      " |  save(self, output_path: Optional[str] = None) -> None\n",
      " |      Saves the class to a given output path.\n",
      " |      \n",
      " |      Args:\n",
      " |          output_path: The location to save - not yet implemented and should\n",
      " |          be self.data_path\n",
      " |      \n",
      " |      Raises:\n",
      " |         NotImplementedError if output_path is not None.\n",
      " |  \n",
      " |  shape(self) -> Tuple[int, List[int]]\n",
      " |      Get the shape of the dataset.\n",
      " |      \n",
      " |      This is the number of entries by the the length of the feature index\n",
      " |      corresponding to that variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The number of elements in the dataset\n",
      " |          A list containing the number of variables for each row.\n",
      " |  \n",
      " |  version(self) -> str\n",
      " |      Returns a version number.\n",
      " |      \n",
      " |      (following <major>.<minor>.<point> convention).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore:\n",
      " |  \n",
      " |  sparsity(self) -> float\n",
      " |      Return the sparsity of the underlying data.\n",
      " |      \n",
      " |      Sparsity is defined as the fraction of zero values in the data.\n",
      " |      It is within the range [0, 1.0]. If there are no values, the\n",
      " |      sparsity is defined as 0.0.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from abc.ABCMeta\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(SingleCellMemMapDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileNames(str, Enum):\n",
    "    \"\"\"Names of files that are generated in SingleCellCollection.\"\"\"\n",
    "\n",
    "    DATA = \"data.npy\"\n",
    "    COLPTR = \"col_ptr.npy\"\n",
    "    ROWPTR = \"row_ptr.npy\"\n",
    "    METADATA = \"metadata.json\"\n",
    "    DTYPE = \"dtypes.json\"\n",
    "    FEATURES = \"features\"\n",
    "    VERSION = \"version.json\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disk_size(directory):\n",
    "    result = subprocess.run(['du', '-sb', directory], stdout=subprocess.PIPE, text=True)\n",
    "    size_in_bytes = int(result.stdout.split()[0])\n",
    "    return size_in_bytes\n",
    "def timeit(method):\n",
    "    @wraps(method)\n",
    "    def timed(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = method(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        run_time = end_time - start_time\n",
    "        print(f\"Method {method.__name__} took {run_time:.4f} seconds\")\n",
    "        return result, run_time\n",
    "    return timed\n",
    "\n",
    "def time_all_methods(cls):\n",
    "    for attr_name, attr_value in cls.__dict__.items():\n",
    "        if callable(attr_value) and attr_name != \"__init__\":  # Check if the attribute is a method\n",
    "            setattr(cls, attr_name, timeit(attr_value))\n",
    "    return cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time_all_methods\n",
    "class AnnDataMetrics:\n",
    "    def __init__(self, adatapath):\n",
    "        self.adatapath = adatapath\n",
    "    def load_backed(self):\n",
    "        self.adata_backed = ad.read_h5ad(self.adatapath, backed=True)\n",
    "\n",
    "    def load_whole(self):\n",
    "        self.adata = ad.read_h5ad(self.adatapath, backed=False)\n",
    "\n",
    "    def max(self):\n",
    "        return self.adata.X.data.max()\n",
    "\n",
    "    def min(self):\n",
    "        return self.adata.X.data.min()\n",
    "\n",
    "    def mean(self):\n",
    "        return self.adata.X.data.mean()\n",
    "\n",
    "    def median(self):\n",
    "        return np.median(self.adata.X.data)\n",
    "\n",
    "    def interQuartRange(self):\n",
    "        return iqr(self.adata.X.data)\n",
    "\n",
    "    def num_values(self):\n",
    "        return self.adata.X.shape[0] * self.adata.X.shape[1]\n",
    "    \n",
    "    def data_type(self):\n",
    "        return self.adata.X.data.dtype\n",
    "    \n",
    "    def get_int_size(self, val): \n",
    "        if val <= 8:\n",
    "            return \"uint8\"\n",
    "        elif val <= 16:\n",
    "            return \"uint16\"\n",
    "        elif val <= 32:\n",
    "            return \"uint32\"\n",
    "        else:\n",
    "            return \"uint64\"\n",
    "\n",
    "    def row_width(self):\n",
    "        return self.get_int_size(math.log2(self.adata.X.indptr[-1]))\n",
    "    \n",
    "    def column_width(self):\n",
    "        return self.get_int_size(math.log2(self.adata.X.indices[-1]))\n",
    "    \n",
    "    def sparsity_stats(self):\n",
    "        num_non_zero = 0\n",
    "        # Get the number of non-zero values\n",
    "        if issparse(self.adata.X):\n",
    "            num_non_zero = self.adata.X.nnz\n",
    "        else:\n",
    "            num_non_zero = (self.adata.X.round() != 0).sum()\n",
    "            \n",
    "        num_vals = self.num_values(self.adata)\n",
    "        num_zeros = num_vals - num_non_zero\n",
    "        sparsity = num_zeros / num_vals\n",
    "        \n",
    "        return num_zeros, num_non_zero, sparsity\n",
    "\n",
    "    def size_disk_bytes(self):\n",
    "        return get_disk_size(self.adatapath)\n",
    "    def size_adata_bytes(self): \n",
    "        return sys.getsizeof(self.adata)\n",
    "    def size_adata_backed_bytes(self): \n",
    "        return sys.getsizeof(self.adata_backed)\n",
    "\n",
    "    def random_rows_whole(self, random_samples = 10_000):\n",
    "        L = self.adata.X.shape[0]     \n",
    "        rIdx = np.sort(np.random.choice(L, size=(random_samples,), replace=True))\n",
    "        x = self.adata[rIdx, :].X\n",
    "        return x\n",
    "    def random_rows_backed(self, random_samples = 10_000):\n",
    "        L = self.adata_backed.X.shape[0]     \n",
    "        rIdx = np.sort(np.random.choice(L, size=(random_samples,), replace=True))\n",
    "        x = self.adata[rIdx, :].X\n",
    "        return x\n",
    "    def random_values_whole(self, random_samples = 10_000):\n",
    "        rows = self.adata.X.shape[0]\n",
    "        cols = self.adata.X.shape[1]\n",
    "        rIdx = np.sort(np.random.choice(rows, size=(random_samples), replace=True))\n",
    "        cIdx = np.sort(np.random.choice(cols, size=(random_samples), replace=True))\n",
    "        return [self.adata.X[r,c] for r,c in zip(rIdx, cIdx)]\n",
    "    \n",
    "    def random_values_backed(self, random_samples = 10_000):\n",
    "        rows = self.adata.X.shape[0]\n",
    "        cols = self.adata.X.shape[1]\n",
    "        rIdx = np.sort(np.random.choice(rows, size=(random_samples), replace=True))\n",
    "        cIdx = np.sort(np.random.choice(cols, size=(random_samples), replace=True))\n",
    "        return [self.adata_backed.X[r,c] for r,c in zip(rIdx, cIdx)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@time_all_methods\n",
    "class SCDLMetrics:\n",
    "    def __init__(self, adatapath, memmap_dir, row_width, column_width, data_type):\n",
    "        self.adatapath = adatapath\n",
    "        self.memmap_dir = memmap_dir\n",
    "        self.row_width = row_width\n",
    "        self.column_width = column_width\n",
    "        self.data_type = data_type\n",
    "        \n",
    "    def create_from_adata(self):\n",
    "        self.first_ds = SingleCellMemMapDataset(self.memmap_dir, \n",
    "                                                self.adatapath,\n",
    "                                                        )\n",
    "\n",
    "    def save(self):\n",
    "        self.first_ds.save()\n",
    "    def load_backed(self):\n",
    "        self.ds = SingleCellMemMapDataset(self.memmap_dir)\n",
    "    def max(self):\n",
    "        return np.max(self.ds.data)\n",
    "\n",
    "    def min(self):\n",
    "        return np.min(self.ds.data)\n",
    "\n",
    "    def mean(self):\n",
    "        return np.mean(self.ds.data)\n",
    "\n",
    "    def median(self):\n",
    "        return np.median(self.ds.data)\n",
    "\n",
    "    def interQuartRange(self):\n",
    "        return iqr(self.ds.data)\n",
    "\n",
    "    def num_values(self):\n",
    "        return self.ds.number_of_values()\n",
    "\n",
    "    def sparsity_stats(self):\n",
    "        return self.ds.sparsity()\n",
    "    \n",
    "    \n",
    "    def size_disk_bytes(self):\n",
    "        return get_disk_size(self.memmap_dir)\n",
    "    def size_mem_dataset_bytes(self): \n",
    "        return sys.getsizeof(self.ds)\n",
    "\n",
    "    def random_rows(self, random_samples = 10_000):\n",
    "        L = self.ds.number_of_rows()   \n",
    "        rIdx = np.sort(np.random.choice(L, size=(random_samples), replace=True))\n",
    "        return [self.ds[v] for v in rIdx]\n",
    "    \n",
    "    def random_values(self, random_samples = 10_000):\n",
    "        rows = self.ds.number_of_rows() \n",
    "        cols = self.ds.shape()[1][0]  \n",
    "        rIdx = np.sort(np.random.choice(rows, size=(random_samples), replace=True))\n",
    "        cIdx = np.sort(np.random.choice(cols, size=(random_samples), replace=True))\n",
    "        return [self.ds.get_row_column(r,c) for r,c in zip(rIdx, cIdx)]\n",
    "\n",
    "    def iterate_dl(self, batch_size = 8):\n",
    "        model = lambda x : x\n",
    "\n",
    "        dataloader = DataLoader(self.ds, batch_size=batch_size, shuffle=True, collate_fn=collate_sparse_matrix_batch)\n",
    "        n_epochs = 1\n",
    "        for e in range(n_epochs):\n",
    "            for batch in dataloader:\n",
    "                model(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method load_backed took 0.2145 seconds\n",
      "Method load_whole took 1.4693 seconds\n",
      "Method max took 0.0050 seconds\n",
      "Method min took 0.0049 seconds\n",
      "Method mean took 0.0057 seconds\n",
      "Method size_disk_bytes took 0.0016 seconds\n",
      "Method size_disk_bytes took 0.0018 seconds\n",
      "Method size_adata_bytes took 0.0155 seconds\n",
      "Method size_adata_backed_bytes took 0.0150 seconds\n"
     ]
    }
   ],
   "source": [
    "fn = \"97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad\"\n",
    "anndatapath = \"examples/hdf5s/\" + fn\n",
    "results_dict[\"anndata file\"] = fn \n",
    "anndata_m = AnnDataMetrics(anndatapath)\n",
    "results_dict[\"AnnData Dataset Backed Load Time (s)\"] = anndata_m.load_backed()[1]\n",
    "results_dict[\"AnnData Dataset Load Time (s)\"] = anndata_m.load_whole()[1]\n",
    "\n",
    "results_dict[\"AnnData Dataset Max Calculation Time (s)\"] = anndata_m.max()[1]\n",
    "results_dict[\"AnnData Dataset Min Calculation Time (s)\"] = anndata_m.min()[1]\n",
    "results_dict[\"AnnData Dataset Mean Calculation Time (s)\"] = anndata_m.mean()[1]\n",
    "results_dict[\"AnnData Dataset Size on Disk (MB)\"] = anndata_m.size_disk_bytes()[0]/(1_024**2)\n",
    "\n",
    "results_dict[\"AnnData Dataset Size on Disk (MB)\"] = anndata_m.size_disk_bytes()[0]/(1_024**2)\n",
    "results_dict[\"AnnData Dataset Size in Memory (MB)\"] = anndata_m.size_adata_bytes()[0]/(1_024**2)\n",
    "results_dict[\"AnnData Backed Dataset Size in Memory (MB)\"] = anndata_m.size_adata_backed_bytes()[0]/(1_024**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method random_rows_whole took 0.0140 seconds\n",
      "Method random_rows_backed took 0.0122 seconds\n"
     ]
    }
   ],
   "source": [
    "results_dict[\"AnnData Time to retrieve a random batch 100 values loaded in memory (s)\"] = anndata_m.random_values_whole(random_samples = 100)[1]\n",
    "results_dict[\"AnnData Time to retrieve a random batch 100 values backed on disk (s)\"] = anndata_m.random_values_backed(random_samples = 100)[1]\n",
    "results_dict[\"AnnData Time to retrieve a random batch of 100 cells loaded in memory (s)\"] = anndata_m.random_rows_whole(random_samples = 100)[1]\n",
    "results_dict[\"AnnData Time to retrieve a random batch of 100 cells backed on disk (s)\"] = anndata_m.random_rows_backed(random_samples = 100)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method data_type took 0.0000 seconds\n",
      "Method get_int_size took 0.0000 seconds\n",
      "Method row_width took 0.0000 seconds\n",
      "Method get_int_size took 0.0000 seconds\n",
      "Method column_width took 0.0000 seconds\n"
     ]
    }
   ],
   "source": [
    "data_type = str(anndata_m.data_type()[0])\n",
    "results_dict[\"data type\"] = data_type\n",
    "row_width = str(anndata_m.row_width()[0][0])\n",
    "column_width = str(anndata_m.column_width()[0][0])\n",
    "results_dict[\"row width\"] = row_width\n",
    "results_dict[\"column width\"] = column_width\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method create_from_adata took 1.8452 seconds\n",
      "Method save took 0.0277 seconds\n"
     ]
    }
   ],
   "source": [
    "scdl_path = \"memmap_93bc4573\"\n",
    "scdl_m = SCDLMetrics(memmap_dir=scdl_path, adatapath=anndatapath, row_width = row_width, column_width = column_width, data_type = data_type)\n",
    "results_dict[\"SCDL Dataset from AnnData Time (s)\"] = scdl_m.create_from_adata()[1]\n",
    "results_dict[\"SCDL Dataset save time (s)\"] = scdl_m.save()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method load_backed took 0.0425 seconds\n",
      "Method max took 0.0090 seconds\n",
      "Method min took 0.0047 seconds\n",
      "Method mean took 0.0055 seconds\n",
      "Method sparsity_stats took 0.0000 seconds\n",
      "Method size_disk_bytes took 0.0019 seconds\n",
      "Method size_mem_dataset_bytes took 0.0000 seconds\n",
      "Method random_rows took 0.1953 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pbinder/bionemo-fw-ea/sub-packages/bionemo-scdl/src/bionemo/scdl/util/torch_dataloader_utils.py:39: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  batch_sparse_tensor = torch.sparse_csr_tensor(batch_rows, batch_cols, batch_values, size=(len(batch), max_pointer))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method iterate_dl took 0.6436 seconds\n"
     ]
    }
   ],
   "source": [
    "results_dict[\"SCDL Dataset Load Time (s)\"] = scdl_m.load_backed()[1]\n",
    "results_dict[\"SCDL Dataset Max Calculation Time (s)\"] = scdl_m.max()[1]\n",
    "results_dict[\"SCDL Dataset Min Calculation Time (s)\"] = scdl_m.min()[1]\n",
    "results_dict[\"SCDL Dataset Mean Calculation Time (s)\"] = scdl_m.mean()[1]\n",
    "results_dict[\"Dataset Sparsity\"] = scdl_m.sparsity_stats()[0]\n",
    "\n",
    "results_dict[\"SCDL Dataset Size on Disk (MB)\"] = scdl_m.size_disk_bytes()[0]/(1_024**2)\n",
    "results_dict[\"SCDL Dataset Size in Memory (MB)\"] = scdl_m.size_mem_dataset_bytes()[0]/(1_024**2)\n",
    "\n",
    "\n",
    "results_dict[\"SCDL Time to retrieve a random batch of 100 cells backed on disk (s)\"] =  scdl_m.random_rows()[1]\n",
    "results_dict[\"SCDL Time to retrive 100 values (s)\"]  = scdl_m.random_values(random_samples = 100)\n",
    "results_dict[\"SCDL Time to iterate over Dataset (s)\"] = scdl_m.iterate_dl()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anndata file': '97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad',\n",
       " 'AnnData Dataset Backed Load Time (s)': 0.21448874473571777,\n",
       " 'AnnData Dataset Load Time (s)': 1.4693443775177002,\n",
       " 'AnnData Dataset Max Calculation Time (s)': 0.0050313472747802734,\n",
       " 'AnnData Dataset Min Calculation Time (s)': 0.004914045333862305,\n",
       " 'AnnData Dataset Mean Calculation Time (s)': 0.005697011947631836,\n",
       " 'AnnData Dataset Size on Disk (MB)': 144.6051368713379,\n",
       " 'AnnData Dataset Size in Memory (MB)': 235.33053874969482,\n",
       " 'AnnData Backed Dataset Size in Memory (MB)': 29.642348289489746,\n",
       " 'AnnData Time to retrieve a random batch of 100 cells loaded in memory (s)': 0.014048576354980469,\n",
       " 'AnnData Time to retrieve a random batch of 100 cells backed on disk (s)': 0.012162923812866211,\n",
       " 'data type': 'float32',\n",
       " 'row width': 'uint32',\n",
       " 'column width': 'uint16',\n",
       " 'SCDL Dataset from AnnData Time (s)': 1.8451993465423584,\n",
       " 'SCDL Dataset save time (s)': 0.027729034423828125,\n",
       " 'SCDL Dataset Load Time (s)': 0.04253530502319336,\n",
       " 'SCDL Dataset Max Calculation Time (s)': 0.0090484619140625,\n",
       " 'SCDL Dataset Min Calculation Time (s)': 0.004740476608276367,\n",
       " 'SCDL Dataset Mean Calculation Time (s)': 0.00553131103515625,\n",
       " 'Dataset Sparsity': np.float64(0.9691868030117566),\n",
       " 'SCDL Dataset Size on Disk (MB)': 207.23700714111328,\n",
       " 'SCDL Dataset Size in Memory (MB)': 4.57763671875e-05,\n",
       " 'SCDL Time to retrieve a random batch of 100 cells backed on disk (s)': 0.1952507495880127,\n",
       " 'SCDL Time to iterate over Dataset (s)': 0.6435585021972656}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = [results_dict]\n",
    "combined = {key: [d[key] for d in dicts] for key in dicts[0]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>anndata file</th>\n",
       "      <th>AnnData Dataset Backed Load Time (s)</th>\n",
       "      <th>AnnData Dataset Load Time (s)</th>\n",
       "      <th>AnnData Dataset Max Calculation Time (s)</th>\n",
       "      <th>AnnData Dataset Min Calculation Time (s)</th>\n",
       "      <th>AnnData Dataset Mean Calculation Time (s)</th>\n",
       "      <th>AnnData Dataset Size on Disk (MB)</th>\n",
       "      <th>AnnData Dataset Size in Memory (MB)</th>\n",
       "      <th>AnnData Backed Dataset Size in Memory (MB)</th>\n",
       "      <th>...</th>\n",
       "      <th>SCDL Dataset Load Time (s)</th>\n",
       "      <th>SCDL Dataset Max Calculation Time (s)</th>\n",
       "      <th>SCDL Dataset Min Calculation Time (s)</th>\n",
       "      <th>SCDL Dataset Mean Calculation Time (s)</th>\n",
       "      <th>Dataset Sparsity</th>\n",
       "      <th>SCDL Dataset Size on Disk (MB)</th>\n",
       "      <th>SCDL Dataset Size in Memory (MB)</th>\n",
       "      <th>SCDL Time to retrieve a random batch of 1000 cells backed on disk (s)</th>\n",
       "      <th>SCDL Time to retrive 1000 values (s)</th>\n",
       "      <th>SCDL Time to iterate over Dataset (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5d6308fd-76e2-45b1-b7a1-1f671e2097b7</td>\n",
       "      <td>0.400354</td>\n",
       "      <td>6.339357</td>\n",
       "      <td>0.021927</td>\n",
       "      <td>0.021588</td>\n",
       "      <td>0.023746</td>\n",
       "      <td>913.051188</td>\n",
       "      <td>965.713401</td>\n",
       "      <td>52.495967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061664</td>\n",
       "      <td>0.036766</td>\n",
       "      <td>0.020856</td>\n",
       "      <td>0.023929</td>\n",
       "      <td>0.942648</td>\n",
       "      <td>916.956355</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.033606</td>\n",
       "      <td>0.66708</td>\n",
       "      <td>1.072148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                          anndata file  \\\n",
       "0           0  5d6308fd-76e2-45b1-b7a1-1f671e2097b7   \n",
       "\n",
       "   AnnData Dataset Backed Load Time (s)  AnnData Dataset Load Time (s)  \\\n",
       "0                              0.400354                       6.339357   \n",
       "\n",
       "   AnnData Dataset Max Calculation Time (s)  \\\n",
       "0                                  0.021927   \n",
       "\n",
       "   AnnData Dataset Min Calculation Time (s)  \\\n",
       "0                                  0.021588   \n",
       "\n",
       "   AnnData Dataset Mean Calculation Time (s)  \\\n",
       "0                                   0.023746   \n",
       "\n",
       "   AnnData Dataset Size on Disk (MB)  AnnData Dataset Size in Memory (MB)  \\\n",
       "0                         913.051188                           965.713401   \n",
       "\n",
       "   AnnData Backed Dataset Size in Memory (MB)  ...  \\\n",
       "0                                   52.495967  ...   \n",
       "\n",
       "   SCDL Dataset Load Time (s)  SCDL Dataset Max Calculation Time (s)  \\\n",
       "0                    0.061664                               0.036766   \n",
       "\n",
       "   SCDL Dataset Min Calculation Time (s)  \\\n",
       "0                               0.020856   \n",
       "\n",
       "  SCDL Dataset Mean Calculation Time (s) Dataset Sparsity  \\\n",
       "0                               0.023929         0.942648   \n",
       "\n",
       "  SCDL Dataset Size on Disk (MB)  SCDL Dataset Size in Memory (MB)  \\\n",
       "0                     916.956355                          0.000046   \n",
       "\n",
       "   SCDL Time to retrieve a random batch of 1000 cells backed on disk (s)  \\\n",
       "0                                           0.033606                       \n",
       "\n",
       "   SCDL Time to retrive 1000 values (s)  SCDL Time to iterate over Dataset (s)  \n",
       "0                               0.66708                               1.072148  \n",
       "\n",
       "[1 rows x 28 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
