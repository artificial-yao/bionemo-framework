{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "from scipy.stats import iqr\n",
    "from bionemo.scdl.io.single_cell_memmap_dataset import SingleCellMemMapDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import wraps\n",
    "import signal\n",
    "\n",
    "import subprocess\n",
    "from bionemo.scdl.util.torch_dataloader_utils import collate_sparse_matrix_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disk_size(directory):\n",
    "    result = subprocess.run(['du', '-sb', directory], stdout=subprocess.PIPE, text=True)\n",
    "    size_in_bytes = int(result.stdout.split()[0])\n",
    "    return size_in_bytes\n",
    "def timeit(method):\n",
    "    @wraps(method)\n",
    "    def timed(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = method(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        run_time = end_time - start_time\n",
    "        print(f\"Method {method.__name__} took {run_time:.4f} seconds\")\n",
    "        return result, run_time\n",
    "    return timed\n",
    "\n",
    "def time_all_methods(cls):\n",
    "    for attr_name, attr_value in cls.__dict__.items():\n",
    "        if callable(attr_value) and attr_name != \"__init__\":  # Check if the attribute is a method\n",
    "            setattr(cls, attr_name, timeit(attr_value))\n",
    "    return cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time_all_methods\n",
    "class AnnDataMetrics:\n",
    "    def __init__(self, adatapath):\n",
    "        self.adatapath = adatapath\n",
    "    def load_backed(self):\n",
    "        self.adata_backed = ad.read_h5ad(self.adatapath, backed=True)\n",
    "\n",
    "    def load_whole(self):\n",
    "        self.adata = ad.read_h5ad(self.adatapath, backed=False)\n",
    "\n",
    "    def max(self):\n",
    "        return self.adata.X.data.max()\n",
    "\n",
    "    def min(self):\n",
    "        return self.adata.X.data.min()\n",
    "\n",
    "    def mean(self):\n",
    "        return self.adata.X.data.mean()\n",
    "\n",
    "    def median(self):\n",
    "        return np.median(self.adata.X.data)\n",
    "\n",
    "    def interQuartRange(self):\n",
    "        return iqr(self.adata.X.data)\n",
    "\n",
    "    def num_values(self):\n",
    "        return self.adata.X.shape[0] * self.adata.X.shape[1]\n",
    "\n",
    "    def sparsity_stats(self):\n",
    "        num_non_zero = 0\n",
    "        # Get the number of non-zero values\n",
    "        if issparse(self.adata.X):\n",
    "            num_non_zero = self.adata.X.nnz\n",
    "        else:\n",
    "            num_non_zero = (self.adata.X.round() != 0).sum()\n",
    "            \n",
    "        num_vals = self.num_values(self.adata)\n",
    "        num_zeros = num_vals - num_non_zero\n",
    "        sparsity = num_zeros / num_vals\n",
    "        \n",
    "        return num_zeros, num_non_zero, sparsity\n",
    "\n",
    "    def size_disk_bytes(self):\n",
    "        return get_disk_size(self.adatapath)\n",
    "    def size_adata_bytes(self): \n",
    "        return sys.getsizeof(self.adata)\n",
    "    def size_adata_backed_bytes(self): \n",
    "        return sys.getsizeof(self.adata_backed)\n",
    "\n",
    "    def random_rows_whole(self, random_samples = 10_000):\n",
    "        L = self.adata.X.shape[0]     \n",
    "        rIdx = np.sort(np.random.choice(L, size=(random_samples,), replace=True))\n",
    "        x = self.adata[rIdx, :].X\n",
    "        return x\n",
    "    def random_rows_backed(self, random_samples = 10_000):\n",
    "        L = self.adata_backed.X.shape[0]     \n",
    "        rIdx = np.sort(np.random.choice(L, size=(random_samples,), replace=True))\n",
    "        x = self.adata[rIdx, :].X\n",
    "        return x\n",
    "    def random_values_whole(self, random_samples = 10_000):\n",
    "        rows = self.adata.X.shape[0]\n",
    "        cols = self.adata.X.shape[1]\n",
    "        rIdx = np.sort(np.random.choice(rows, size=(random_samples), replace=True))\n",
    "        cIdx = np.sort(np.random.choice(cols, size=(random_samples), replace=True))\n",
    "        return [self.adata.X[r,c] for r,c in zip(rIdx, cIdx)]\n",
    "    \n",
    "    def random_values_backed(self, random_samples = 10_000):\n",
    "        rows = self.adata.X.shape[0]\n",
    "        cols = self.adata.X.shape[1]\n",
    "        rIdx = np.sort(np.random.choice(rows, size=(random_samples), replace=True))\n",
    "        cIdx = np.sort(np.random.choice(cols, size=(random_samples), replace=True))\n",
    "        return [self.adata_backed.X[r,c] for r,c in zip(rIdx, cIdx)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time_all_methods\n",
    "class SCDLMetrics:\n",
    "    def __init__(self, adatapath, memmap_dir):\n",
    "        self.adatapath = adatapath\n",
    "        self.memmap_dir = memmap_dir\n",
    "    \n",
    "    def create_from_adata(self):\n",
    "        self.first_ds = SingleCellMemMapDataset(self.memmap_dir, self.adatapath)\n",
    "    def save(self):\n",
    "        self.first_ds.save()\n",
    "    def load_backed(self):\n",
    "        self.ds = SingleCellMemMapDataset(self.memmap_dir)\n",
    "    def max(self):\n",
    "        return np.max(self.ds.data)\n",
    "\n",
    "    def min(self):\n",
    "        return np.min(self.ds.data)\n",
    "\n",
    "    def mean(self):\n",
    "        return np.mean(self.ds.data)\n",
    "\n",
    "    def median(self):\n",
    "        return np.median(self.ds.data)\n",
    "\n",
    "    def interQuartRange(self):\n",
    "        return iqr(self.ds.data)\n",
    "\n",
    "    def num_values(self):\n",
    "        return self.ds.number_of_values()\n",
    "\n",
    "    def sparsity_stats(self):\n",
    "        return self.ds.sparsity()\n",
    "    \n",
    "    \n",
    "    def size_disk_bytes(self):\n",
    "        return get_disk_size(self.memmap_dir)\n",
    "    def size_mem_dataset_bytes(self): \n",
    "        return sys.getsizeof(self.ds)\n",
    "\n",
    "    def random_rows(self, random_samples = 10_000):\n",
    "        L = self.ds.number_of_rows()   \n",
    "        rIdx = np.sort(np.random.choice(L, size=(random_samples), replace=True))\n",
    "        return [self.ds[v] for v in rIdx]\n",
    "    \n",
    "    def random_values(self, random_samples = 10_000):\n",
    "        rows = self.ds.number_of_rows() \n",
    "        cols = self.ds.shape()[1][0]  \n",
    "        rIdx = np.sort(np.random.choice(rows, size=(random_samples), replace=True))\n",
    "        cIdx = np.sort(np.random.choice(cols, size=(random_samples), replace=True))\n",
    "        return [self.ds.get_row_column(r,c) for r,c in zip(rIdx, cIdx)]\n",
    "\n",
    "    def iterate_dl(self, batch_size = 8):\n",
    "        model = lambda x : x\n",
    "\n",
    "        dataloader = DataLoader(self.ds, batch_size=batch_size, shuffle=True, collate_fn=collate_sparse_matrix_batch)\n",
    "        n_epochs = 1\n",
    "        for e in range(n_epochs):\n",
    "            for batch in dataloader:\n",
    "                model(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method load_backed took 0.8858 seconds\n",
      "Method load_whole took 1.2230 seconds\n",
      "Method max took 0.0036 seconds\n",
      "Method min took 0.0030 seconds\n",
      "Method mean took 0.0038 seconds\n",
      "Method size_disk_bytes took 0.0027 seconds\n",
      "Method size_disk_bytes took 0.0030 seconds\n",
      "Method size_adata_bytes took 0.0468 seconds\n",
      "Method size_adata_backed_bytes took 0.0462 seconds\n"
     ]
    }
   ],
   "source": [
    "fn = \"06a7ffec-2697-4d6f-96f6-d00a34bedb3d.h5ad\"\n",
    "anndatapath = \"examples/hdf5s/\" + fn\n",
    "results_dict[\"anndata file\"] = fn \n",
    "anndata_m = AnnDataMetrics(anndatapath)\n",
    "results_dict[\"AnnData Dataset Backed Load Time (s)\"] = anndata_m.load_backed()[1]\n",
    "results_dict[\"AnnData Dataset Load Time (s)\"] = anndata_m.load_whole()[1]\n",
    "\n",
    "results_dict[\"AnnData Dataset Max Calculation Time (s)\"] = anndata_m.max()[1]\n",
    "results_dict[\"AnnData Dataset Min Calculation Time (s)\"] = anndata_m.min()[1]\n",
    "results_dict[\"AnnData Dataset Mean Calculation Time (s)\"] = anndata_m.mean()[1]\n",
    "results_dict[\"AnnData Dataset Size on Disk (MB)\"] = anndata_m.size_disk_bytes()[0]/(1_024**2)\n",
    "\n",
    "results_dict[\"AnnData Dataset Size on Disk (MB)\"] = anndata_m.size_disk_bytes()[0]/(1_024**2)\n",
    "results_dict[\"AnnData Dataset Size in Memory (MB)\"] = anndata_m.size_adata_bytes()[0]/(1_024**2)\n",
    "results_dict[\"AnnData Backed Dataset Size in Memory (MB)\"] = anndata_m.size_adata_backed_bytes()/(1_024**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method random_rows_whole took 0.0133 seconds\n",
      "Method random_rows_backed took 0.0122 seconds\n"
     ]
    }
   ],
   "source": [
    "#anndata_m.random_values_whole(random_samples = 100)\n",
    "#anndata_m.random_values_backed(random_samples = 100)\n",
    "results_dict[\"AnnData Time to retrieve a random batch of 100 cells loaded in memory (s)\"] = anndata_m.random_rows_whole(random_samples = 100)[1]\n",
    "results_dict[\"AnnData Time to retrieve a random batch of 100 cells backed on disk (s)\"] = anndata_m.random_rows_backed(random_samples = 100)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method __init__ took 0.0000 seconds\n",
      "Method create_from_adata took 1.5763 seconds\n",
      "Method save took 0.0050 seconds\n"
     ]
    }
   ],
   "source": [
    "scdl_path = \"memmap_93bc4573\"\n",
    "scdl_m = SCDLMetrics(memmap_dir=scdl_path, adatapath=anndatapath)\n",
    "results_dict[\"SCDL Dataset from AnnData Time (s)\"] = scdl_m.create_from_adata()[1]\n",
    "results_dict[\"SCDL Dataset save time (s)\"] = scdl_m.save()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method load_backed took 0.0623 seconds\n",
      "Method max took 0.0073 seconds\n",
      "Method min took 0.0035 seconds\n",
      "Method mean took 0.0043 seconds\n",
      "Method sparsity_stats took 0.0000 seconds\n",
      "Method size_disk_bytes took 0.0031 seconds\n",
      "Disk size: 161.6428394317627 MB\n",
      "Method size_mem_dataset_bytes took 0.0000 seconds\n",
      "SCDataset size: 4.57763671875e-05 MB\n",
      "Method random_rows took 0.0989 seconds\n",
      "Method random_values took 0.0020 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pbinder/miniforge3/envs/newenv10/lib/python3.10/site-packages/bionemo/scdl/util/torch_dataloader_utils.py:39: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  batch_sparse_tensor = torch.sparse_csr_tensor(batch_rows, batch_cols, batch_values, size=(len(batch), max_pointer))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method iterate_dl took 6.1821 seconds\n"
     ]
    }
   ],
   "source": [
    "results_dict[\"SCDL Dataset Load Time (s)\"] = scdl_m.load_backed()[1]\n",
    "results_dict[\"SCDL Dataset Max Calculation Time (s)\"] = scdl_m.max()[1]\n",
    "results_dict[\"SCDL Dataset Min Calculation Time (s)\"] = scdl_m.min()[1]\n",
    "results_dict[\"SCDL Dataset Mean Calculation Time (s)\"] = scdl_m.mean()[1]\n",
    "results_dict[\"Dataset Sparsity\"] = scdl_m.sparsity_stats()[0]\n",
    "\n",
    "results_dict[\"SCDL Dataset Size on Disk (MB)\"] = scdl_m.size_disk_bytes()[0]/(1_024**2)\n",
    "results_dict[\"SCDL Dataset Size in Memory (MB)\"] = scdl_m.size_mem_dataset_bytes()[0]/(1_024**2)\n",
    "\n",
    "\n",
    "results_dict[\"SCDL Time to retrieve a random batch of 100 cells backed on disk (s)\"] =  scdl_m.random_rows()[1]\n",
    "#y = scdl_m.random_values(random_samples = 100)\n",
    "results_dict[\"SCDL Time to iterate over Dataset (s)\"] = scdl_m.iterate_dl()[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
