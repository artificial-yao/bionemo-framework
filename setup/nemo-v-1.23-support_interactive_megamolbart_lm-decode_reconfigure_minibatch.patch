diff --git a/nemo/collections/nlp/models/language_modeling/megatron_lm_encoder_decoder_model.py b/nemo/collections/nlp/models/language_modeling/megatron_lm_encoder_decoder_model.py
index fd0381502..538c18352 100644
--- a/nemo/collections/nlp/models/language_modeling/megatron_lm_encoder_decoder_model.py
+++ b/nemo/collections/nlp/models/language_modeling/megatron_lm_encoder_decoder_model.py
@@ -1129,6 +1129,7 @@ class MegatronLMEncoderDecoderModel(MegatronBaseModel):
         batch_data=None,
         sampling_method: str = "greedy-search",
         sampling_kwargs: dict = {},
+        reconfigure_microbatch: bool = True,
     ):
         """
         tokens_enc - a tensor of shape [batch_size, seq_len] that contains the input tokens.
@@ -1177,13 +1178,14 @@ class MegatronLMEncoderDecoderModel(MegatronBaseModel):
             self.trainer.strategy.setup_environment()

             # Reconfigure microbatch sizes here because on model restore, this will contain the micro/global batch configuration used while training.
-            _reconfigure_microbatch_calculator(
-                rank=0,  # This doesn't matter since it is only used for logging
-                rampup_batch_size=None,
-                global_batch_size=1,
-                micro_batch_size=1,  # Make sure that there is no "grad acc" while decoding.
-                data_parallel_size=1,  # We check above to make sure that dataparallel size is always 1 at inference.
-            )
+            if reconfigure_microbatch:
+                _reconfigure_microbatch_calculator(
+                    rank=0,  # This doesn't matter since it is only used for logging
+                    rampup_batch_size=None,
+                    global_batch_size=1,
+                    micro_batch_size=1,  # Make sure that there is no "grad acc" while decoding.
+                    data_parallel_size=1,  # We check above to make sure that dataparallel size is always 1 at inference.
+                )

         # If classes that inherit from this class are using a different tokenizer,
         tokenizer = self.tokenizer if tokenizer is None else tokenizer
@@ -1201,13 +1203,14 @@ class MegatronLMEncoderDecoderModel(MegatronBaseModel):
         # Reconfigure microbatch calculator here to set num microbatches to 1 while decoding since its not clear how to decode with "grad acc".
         # reconfigure back to how things were before decode
         # TODO: Check if the user is trying to do gradient acc and maybe throw error
-        _reconfigure_microbatch_calculator(
-            rank=app_state.global_rank,
-            rampup_batch_size=None,
-            global_batch_size=global_batch_per_gpu * parallel_state.get_data_parallel_world_size(),
-            micro_batch_size=global_batch_per_gpu,  # Make sure that there is no "grad acc" while decoding.
-            data_parallel_size=parallel_state.get_data_parallel_world_size(),
-        )
+        if reconfigure_microbatch:
+            _reconfigure_microbatch_calculator(
+                rank=app_state.global_rank,
+                rampup_batch_size=None,
+                global_batch_size=global_batch_per_gpu * parallel_state.get_data_parallel_world_size(),
+                micro_batch_size=global_batch_per_gpu,  # Make sure that there is no "grad acc" while decoding.
+                data_parallel_size=parallel_state.get_data_parallel_world_size(),
+            )
         # TODO: Figure out how to handle bos being either <bos> for NeMo-Megatron and <pad> for Huggingface/Google.
         bos_id = tokenizer.bos_id if bos_id is None else bos_id
         # initial prompt can be given
@@ -1224,7 +1227,7 @@ class MegatronLMEncoderDecoderModel(MegatronBaseModel):
             # Encode returns a tensr of shape [batch, seq_len, hidden]
             # All ranks will call `.encode()`, but only the last rank will have a non-empty output tensor.
             enc_output = self.encode(
-                tokens_enc=tokens_enc, enc_mask=enc_mask, encoder_input=encoder_input, reconfigure_microbatch=False
+                tokens_enc=tokens_enc, enc_mask=enc_mask, encoder_input=encoder_input, reconfigure_microbatch=reconfigure_microbatch
             )
         if enc_output_attn_mask is None:
             enc_output_attn_mask = enc_mask
@@ -1289,13 +1292,14 @@ class MegatronLMEncoderDecoderModel(MegatronBaseModel):
                         # reconfigure batch size for apex since the tensor have been augmented with beam size
                         global_batch_per_gpu = token_ids.shape[0]
                         tensor_shape[1] = global_batch_per_gpu
-                        _reconfigure_microbatch_calculator(
-                            rank=app_state.global_rank,
-                            rampup_batch_size=None,
-                            global_batch_size=global_batch_per_gpu * parallel_state.get_data_parallel_world_size(),
-                            micro_batch_size=global_batch_per_gpu,
-                            data_parallel_size=parallel_state.get_data_parallel_world_size(),
-                        )
+                        if reconfigure_microbatch:
+                            _reconfigure_microbatch_calculator(
+                                rank=app_state.global_rank,
+                                rampup_batch_size=None,
+                                global_batch_size=global_batch_per_gpu * parallel_state.get_data_parallel_world_size(),
+                                micro_batch_size=global_batch_per_gpu,
+                                data_parallel_size=parallel_state.get_data_parallel_world_size(),
+                            )

                         # collect all predicted tokens and log_probs
                         predicted_tokens_dec = torch.cat(
@@ -1380,13 +1384,14 @@ class MegatronLMEncoderDecoderModel(MegatronBaseModel):
                 )

         # Reset microbatch calculator to what it was before decoding.
-        _reconfigure_microbatch_calculator(
-            rank=app_state.global_rank,
-            rampup_batch_size=None,
-            global_batch_size=global_batch_per_gpu * parallel_state.get_data_parallel_world_size(),
-            micro_batch_size=global_batch_per_gpu // num_micro_batches_before_decode,
-            data_parallel_size=parallel_state.get_data_parallel_world_size(),
-        )
+        if reconfigure_microbatch:
+            _reconfigure_microbatch_calculator(
+                rank=app_state.global_rank,
+                rampup_batch_size=None,
+                global_batch_size=global_batch_per_gpu * parallel_state.get_data_parallel_world_size(),
+                micro_batch_size=global_batch_per_gpu // num_micro_batches_before_decode,
+                data_parallel_size=parallel_state.get_data_parallel_world_size(),
+            )

         if beam_search and beam_size > 1:
             if keep_only_best_tokens:
